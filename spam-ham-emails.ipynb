{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1603933,"sourceType":"datasetVersion","datasetId":946753},{"sourceId":5260904,"sourceType":"datasetVersion","datasetId":3061553},{"sourceId":6420219,"sourceType":"datasetVersion","datasetId":3703513}],"dockerImageVersionId":30407,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kapusharinka/spam-detection?scriptVersionId=178765087\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import time\nfrom pprint import pprint\nimport numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-19T17:50:31.567022Z","iopub.execute_input":"2024-05-19T17:50:31.567909Z","iopub.status.idle":"2024-05-19T17:50:31.61112Z","shell.execute_reply.started":"2024-05-19T17:50:31.567861Z","shell.execute_reply":"2024-05-19T17:50:31.609341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nspecies = (\"Network logs\", \"Count vectorizer\", \"TF-IDF\", \"Word embeddings\")\npenguin_means = {\n    'The k-nearest neighbors (precision)': (0.99, 0.96, 0.97, 0.97),\n    'The k-nearest neighbors (recall)': (0.99, 0.87, 0.76, 0.99),\n    'Linear support vector (precision)': (0.99, 0.97, 0.98, 0.98),\n    'Linear support vector (recall)': (0.99, 0.99, 0.99, 0.99),\n}\n\nx = np.arange(len(species))  # the label locations\nwidth = 0.15  # the width of the bars\nmultiplier = 0\n\nsns.set_palette(\"Paired\")\nfig, ax = plt.subplots(figsize=(12, 6))\n\nfor attribute, measurement in penguin_means.items():\n    offset = width * multiplier\n    rects = ax.bar(x + offset, measurement, width, label=attribute)\n    ax.bar_label(rects, padding=3)\n    multiplier += 1\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xticks(x + width * (len(penguin_means) - 1) / 2)\nax.set_xticklabels(species)\nax.set_ylim(0, 1.1)\n\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:02:53.672136Z","iopub.execute_input":"2024-05-19T20:02:53.67287Z","iopub.status.idle":"2024-05-19T20:02:54.088743Z","shell.execute_reply.started":"2024-05-19T20:02:53.67281Z","shell.execute_reply":"2024-05-19T20:02:54.086976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Data\nspecies = (\"Network logs\", \"Count vectorizer\", \"TF-IDF\", \"Word embeddings\")\npenguin_means = {\n    'One-class SVM (precision)': (0.98, 0.95, 0.93, 0.93),\n    'One-class SVM (recall)': (0.79, 0.5, 0.48, 0.49),\n    'Local outlier factor (novelty, precision)': (0.98, 0.94, 0.94, 0.89),\n    'Local outlier factor (novelty, recall)': (0.93, 0.87, 0.78, 0.2),\n    'Local outlier factor (outliers, precision)': (0.95, 0.94, 0.94, 0.92),\n    'Local outlier factor (outliers, recall)': (0.95, 0.88, 0.77, 0.19),\n    'Isolation forest (precision)': (0.98, 0.8, 0.01, 0.91),\n    'Isolation forest (recall)': (0.98, 0, 0.01, 0.22),\n    'Gaussian mixture model (precision)': (0.98, 0.95, 0.96, 0.95),\n    'Gaussian mixture model (recall)': (0.98, 0.95, 0.95, 0.95),\n    'The k-means (precision)': (1, 0.98, 0.97, 0.98),\n    'The k-means (recall)': (0.05, 0.05, 0.05, 0.05),\n}\n\nx = np.arange(len(species))  # the label locations\nmultiplier = 0\n\n# Plot setup\nsns.set_palette(\"Paired\")\nfig, ax = plt.subplots()\n\n# Create a line plot\nfor attribute, measurement in penguin_means.items():\n    ax.plot(x, measurement, marker='o', label=attribute)\n\n# Add labels and title\nax.set_xticks(x)\nax.set_xticklabels(species)\nax.set_ylim(0, 1)\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:17:37.040204Z","iopub.execute_input":"2024-05-19T20:17:37.040738Z","iopub.status.idle":"2024-05-19T20:17:37.414409Z","shell.execute_reply.started":"2024-05-19T20:17:37.040692Z","shell.execute_reply":"2024-05-19T20:17:37.413188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Data\nspecies = (\"Network logs\", \"Count vectorizer\", \"TF-IDF\", \"Word embeddings\")\npenguin_means = {\n    'Gaussian Naive Bayes (precision)': (0.99, 0.97, 0.97, 0.98),\n    'Gaussian Naive Bayes (recall)': (0.84, 0.99, 0.99, 0.89),\n    'Logistic regression (novelty, precision)': (0.99, 0.97, 0.98, 0.98),\n    'Logistic regression (novelty, recall)': (0.99, 0.99, 0.99, 0.98),\n    'The k-nearest neighbors (outliers, precision)': (0.99, 0.96, 0.97, 0.97),\n    'The k-nearest neighbors (outliers, recall)': (0.99, 0.87, 0.76, 0.99),\n    'Linear support vector (precision)': (0.99, 0.97, 0.98, 0.98),\n    'Linear support vector (recall)': (0.99, 0.99, 0.99, 0.99),\n    'Decision tree (precision)': (0.99, 0.97, 0.97, 0.97),\n    'Decision tree (recall)': (0.99, 0.97, 0.97, 0.97),\n    'Random forest (precision)': (0.99, 0.97, 0.97, 0.96),\n    'Random forest (recall)': (0.99, 0.99, 0.99, 0.99),\n    'Voting classifier (precision)': (0.99, 0.98, 0.98, 0.98),\n    'Voting classifier (recall)': (0.99, 0.99, 0.99, 0.99),\n}\n\nx = np.arange(len(species))  # the label locations\nmultiplier = 0\n\n# Plot setup\nsns.set_palette(\"Paired\")\nfig, ax = plt.subplots()\n\n# Create a line plot\nfor attribute, measurement in penguin_means.items():\n    ax.plot(x, measurement, marker='o', label=attribute)\n\n# Add labels and title\nax.set_xticks(x)\nax.set_xticklabels(species)\nax.set_ylim(0.75, 1)\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T20:24:33.946048Z","iopub.execute_input":"2024-05-19T20:24:33.947834Z","iopub.status.idle":"2024-05-19T20:24:34.346997Z","shell.execute_reply.started":"2024-05-19T20:24:33.947755Z","shell.execute_reply":"2024-05-19T20:24:34.344853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Means after balancing \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nspecies = (\"Count vectorizer\", \"TF-IDF\", \"Word embeddings\")\npenguin_means = {\n    'Precision (unbalanced)': (0.98, 0.97, 0.98),\n    'Precision (balanced)': (0.54, 0.49, 0.59),\n    'Recall (unbalanced)': (0.05, 0.05, 0.05),\n    'Recall (balanced)': (0.54, 0.49, 0.58),\n}\n\nx = np.arange(len(species))  # the label locations\nwidth = 0.2  # the width of the bars\nmultiplier = 0\n\nsns.set_palette(\"Paired\")\nfig, ax = plt.subplots(layout='constrained')\n\nfor attribute, measurement in penguin_means.items():\n    offset = width * multiplier\n    rects = ax.bar(x + offset, measurement, width, label=attribute)\n    ax.bar_label(rects, padding=3)\n    multiplier += 1\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xticks(x + width, species)\nax.set_ylim(0, 1.1)\n\nbox = ax.get_position()\nax.set_position([box.x0, box.y0 + box.height * 0.1,\n                 box.width, box.height * 0.9])\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n          ncol=5)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:33.361038Z","iopub.execute_input":"2024-05-19T17:50:33.361874Z","iopub.status.idle":"2024-05-19T17:50:33.650817Z","shell.execute_reply.started":"2024-05-19T17:50:33.361829Z","shell.execute_reply":"2024-05-19T17:50:33.649365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Articles about ML\nhttps://github.com/christianversloot/machine-learning-articles/tree/main","metadata":{}},{"cell_type":"markdown","source":"# Data preprocessing\n## Preparing datasets","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/email-spam-dataset/enronSpamSubset.csv')\ndata.info()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:33.652285Z","iopub.execute_input":"2024-05-19T17:50:33.652649Z","iopub.status.idle":"2024-05-19T17:50:34.140903Z","shell.execute_reply.started":"2024-05-19T17:50:33.652613Z","shell.execute_reply":"2024-05-19T17:50:34.139448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete unneeded columns\ndata.drop([\"Unnamed: 0\",\"Unnamed: 0.1\"],inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:34.145189Z","iopub.execute_input":"2024-05-19T17:50:34.145599Z","iopub.status.idle":"2024-05-19T17:50:34.152819Z","shell.execute_reply.started":"2024-05-19T17:50:34.145558Z","shell.execute_reply":"2024-05-19T17:50:34.151767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove emails with body longer than 10000\ndata = data[data['Body'].apply(lambda x: len(str(x)) <= 10000)]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:34.154309Z","iopub.execute_input":"2024-05-19T17:50:34.155104Z","iopub.status.idle":"2024-05-19T17:50:34.182079Z","shell.execute_reply.started":"2024-05-19T17:50:34.155063Z","shell.execute_reply":"2024-05-19T17:50:34.180527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a fraction\n# data = data.sample(frac=0.7)\n# remove missing values (NaN)\ndata.dropna(inplace=True)\ndata.info()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:34.183743Z","iopub.execute_input":"2024-05-19T17:50:34.184528Z","iopub.status.idle":"2024-05-19T17:50:34.220635Z","shell.execute_reply.started":"2024-05-19T17:50:34.184483Z","shell.execute_reply":"2024-05-19T17:50:34.219533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:34.222311Z","iopub.execute_input":"2024-05-19T17:50:34.223008Z","iopub.status.idle":"2024-05-19T17:50:34.257292Z","shell.execute_reply.started":"2024-05-19T17:50:34.222966Z","shell.execute_reply":"2024-05-19T17:50:34.255912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nax = sns.countplot(x=data[\"Label\"], palette=(\"Pastel2\"))\nabs_values = data[\"Label\"].value_counts(ascending=False).values\nax.bar_label(container=ax.containers[0], labels=abs_values)\nax.set_xticklabels(['hams', 'spams'])\n\nplt.xlabel(None)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:34.259585Z","iopub.execute_input":"2024-05-19T17:50:34.260399Z","iopub.status.idle":"2024-05-19T17:50:34.459702Z","shell.execute_reply.started":"2024-05-19T17:50:34.260342Z","shell.execute_reply":"2024-05-19T17:50:34.457883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emails = data[\"Body\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:50:34.462771Z","iopub.execute_input":"2024-05-19T17:50:34.463374Z","iopub.status.idle":"2024-05-19T17:50:34.47013Z","shell.execute_reply.started":"2024-05-19T17:50:34.463314Z","shell.execute_reply":"2024-05-19T17:50:34.468448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_min_avg_len(email_set):\n    max_len, min_len = len(email_set[0]), len(email_set[0])\n    avg_len = 0\n    for email in email_set:\n        if max_len < len(email):\n            max_len = len(email)\n        if min_len > len(email):\n            min_len = len(email)\n        avg_len += len(email)\n    avg_len = avg_len // len(email_set)\n    return f\"maximum length: {max_len}, minimum length: {min_len}, average length: {avg_len}\"","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:57:27.324773Z","iopub.execute_input":"2024-05-19T17:57:27.325269Z","iopub.status.idle":"2024-05-19T17:57:27.334175Z","shell.execute_reply.started":"2024-05-19T17:57:27.325225Z","shell.execute_reply":"2024-05-19T17:57:27.332711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hams = list(emails[data[\"Label\"] == 1])\nspams = list(emails[data[\"Label\"] == 0])\n\n\nprint(\"All emails:\", max_min_avg_len(emails))\nprint(\"Ham:\", max_min_avg_len(hams))\nprint(\"Spam:\", max_min_avg_len(spams))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:57:28.9468Z","iopub.execute_input":"2024-05-19T17:57:28.947299Z","iopub.status.idle":"2024-05-19T17:57:28.973288Z","shell.execute_reply.started":"2024-05-19T17:57:28.947251Z","shell.execute_reply":"2024-05-19T17:57:28.971868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I'm removing unneeded characters, like HTML tags, emails etc.","metadata":{}},{"cell_type":"code","source":"import re\n\n# remove email addresses\nemails = [re.sub('\\S*@\\S*\\s?', '', text) for text in emails]\n# remove url links\nemails = [re.sub('\\S*(http[s]?://|www\\.)\\S*', '', text) for text in emails]\n# remove HTML tags\nemails = [re.sub(r\"'<.*?>'\", \"\", text) for text in emails]\n# remove special characters and numbers\nemails = [re.sub(\"[^a-zA-Z]\",\" \",text) for text in emails]\n# remove too short (2- characters) words\nemails = [re.sub(r\"\\b\\w{1,2}\\b\", \"\",text) for text in emails]\n# and too long (17+ characters) \nemails = [re.sub(r\"\\b\\w{17,}\\b\", \"\",text) for text in emails]\n# remove multiple spaces\nemails = [re.sub(' +', ' ', text).strip() for text in emails]\n# lower\nemails = [text.lower() for text in emails]\n\nemails[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:14:05.945189Z","iopub.execute_input":"2024-05-19T11:14:05.946319Z","iopub.status.idle":"2024-05-19T11:14:10.380688Z","shell.execute_reply.started":"2024-05-19T11:14:05.946258Z","shell.execute_reply":"2024-05-19T11:14:10.378985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization and lemmatization\nTokenization: [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) (page 180)\n<ul>\n    <li>one-hot encoding of tokens</li>\n    <li>token embedding</li>\n</ul>\nFor more, look at <i>Deep Learning</i> section below.","metadata":{}},{"cell_type":"markdown","source":"We can choose between stemming or lemmatization - lemmatizators are slower, but change tenses and nouns. <br/>\nhttps://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n\nFirstly I used NLTK lemmatization, but it is very slow on my computer, so I tried SpaCy instead (https://spacy.io).","metadata":{}},{"cell_type":"code","source":"custom_stopwords = ['subject', 'empty', 'email', 'mail', 'enron', 'linux', 'list', 'get', 'http', 'vince', 'com', 'org', 'www', 'etc', 'ect', 'edu', 'hou', 'would', 'need']","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:14:10.382536Z","iopub.execute_input":"2024-05-19T11:14:10.382981Z","iopub.status.idle":"2024-05-19T11:14:10.389646Z","shell.execute_reply.started":"2024-05-19T11:14:10.382944Z","shell.execute_reply":"2024-05-19T11:14:10.388249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using SpaCy\nhttps://stackoverflow.com/a/75215495","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# add stop words\nfor word in custom_stopwords:\n    nlp.vocab[word].is_stop = True","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:14:10.39154Z","iopub.execute_input":"2024-05-19T11:14:10.392444Z","iopub.status.idle":"2024-05-19T11:14:27.70887Z","shell.execute_reply.started":"2024-05-19T11:14:10.3924Z","shell.execute_reply":"2024-05-19T11:14:27.707345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatization\nemails = [[token.lemma_ for token in nlp(text) if not token.is_stop] for text in emails]\nemails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:14:27.71044Z","iopub.execute_input":"2024-05-19T11:14:27.711423Z","iopub.status.idle":"2024-05-19T11:18:50.136216Z","shell.execute_reply.started":"2024-05-19T11:14:27.711378Z","shell.execute_reply":"2024-05-19T11:18:50.134883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature extraction \nCreating a vector of features (words) for each email. <br />\nOpenAI:\n> Both CountVectorizer and TF-IDF (Term Frequency-Inverse Document Frequency) from scikit-learn are popular techniques for feature extraction in text data like emails, and each has its own merits.\n> \n> CountVectorizer creates a Bag of Words (BoW) model, where the features are the counts of each word in the document. This method is simple and easy to implement but can give more importance to words that appear frequently, regardless of their significance in distinguishing spam from non-spam emails.\n> \n> TF-IDF, on the other hand, takes into account not only the frequency of a word in a document but also its inverse frequency across all documents. This means that words that are common across all emails will receive lower weights, while words that are unique to specific emails will receive higher weights. This can be advantageous for spam detection, as spam emails often contain specific words or phrases that are less common in legitimate emails.\n> \n> In general, TF-IDF tends to work better than CountVectorizer for spam detection because it can better capture the importance of different words. However, the choice between the two methods will depend on the specific characteristics of the dataset and the problem you're trying to solve. It's a good idea to experiment with both techniques and evaluate their performance on your dataset using cross-validation or a separate validation set. This will help you determine which method works best for your particular spam detection task.","metadata":{}},{"cell_type":"code","source":"# bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# ngram_range=(1,2) means that the model will consider bigrams too\n# min_df=0.003 means that the model will not consider rare words\ncount_vectorizer = CountVectorizer(max_features=25000, ngram_range=(1,5), min_df=0.003, max_df=0.9)\nbag_of_words = count_vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(bag_of_words.shape)\ncount_vectorizer.get_feature_names_out()[:10] # first 10 in alphabetical order","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:18:50.137874Z","iopub.execute_input":"2024-05-19T11:18:50.138254Z","iopub.status.idle":"2024-05-19T11:19:06.271696Z","shell.execute_reply.started":"2024-05-19T11:18:50.1382Z","shell.execute_reply":"2024-05-19T11:19:06.270423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_vectorizer = TfidfVectorizer(max_features=25000, ngram_range=(1,5), min_df=0.003, max_df=0.9)\ntf_idf = tf_vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(tf_idf.shape)\ntf_vectorizer.get_feature_names_out()[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:19:06.273334Z","iopub.execute_input":"2024-05-19T11:19:06.273795Z","iopub.status.idle":"2024-05-19T11:19:22.878333Z","shell.execute_reply.started":"2024-05-19T11:19:06.273745Z","shell.execute_reply":"2024-05-19T11:19:22.877022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word embeddings\nA word embedding is a vector, that tries to capture \"meaning\" of a word/sentence/document etc... Some of the algorithms are provided by Word2Vec, GloVe. I used one provided by SpaCy.","metadata":{}},{"cell_type":"markdown","source":"### Using SpaCy\nAlso check [sense2vec](http://https://github.com/explosion/sense2vec)","metadata":{}},{"cell_type":"code","source":"nlp = spacy.blank(\"en\")\nnlp.from_disk('/kaggle/input/en-core-web-lg/en_core_web_lg/en_core_web_lg-3.6.0')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:19:22.880035Z","iopub.execute_input":"2024-05-19T11:19:22.880425Z","iopub.status.idle":"2024-05-19T11:19:29.136497Z","shell.execute_reply.started":"2024-05-19T11:19:22.880385Z","shell.execute_reply":"2024-05-19T11:19:29.135466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating vectors for each email. We take a vector for each word from email, and finding the average vector for a whole email.","metadata":{}},{"cell_type":"code","source":"email_embeddings = []\nfor email in emails:\n    if len(email) > 0:\n        spacy_doc = nlp(' '.join(email))\n        avg_vector = sum([token.vector for token in spacy_doc]) / len(email)\n    else:\n        avg_vector = nlp('').vector\n    email_embeddings.append(avg_vector)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:19:29.137807Z","iopub.execute_input":"2024-05-19T11:19:29.138905Z","iopub.status.idle":"2024-05-19T11:19:36.935368Z","shell.execute_reply.started":"2024-05-19T11:19:29.13884Z","shell.execute_reply":"2024-05-19T11:19:36.934278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word cloud\nHere are word clouds for spams and hams with the most frequent words, created with TF-IDF vectorizer.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\ndata['Tokens'] = emails\n\n# spams\nspams = data.loc[data['Label'] == 1, ['Tokens']]\nspam_x = tf_vectorizer.fit_transform([\" \".join(text) for text in spams['Tokens']]).toarray()\n\ndf = pd.DataFrame(spam_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\ndf.head(10)\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:19:36.936896Z","iopub.execute_input":"2024-05-19T11:19:36.938122Z","iopub.status.idle":"2024-05-19T11:20:05.447721Z","shell.execute_reply.started":"2024-05-19T11:19:36.938075Z","shell.execute_reply":"2024-05-19T11:20:05.446305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hams\nhams = data.loc[data['Label'] == 0, ['Tokens']]\nham_x = tf_vectorizer.fit_transform([\" \".join(text) for text in hams['Tokens']]).toarray()\n\ndf = pd.DataFrame(ham_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:20:05.449283Z","iopub.execute_input":"2024-05-19T11:20:05.449806Z","iopub.status.idle":"2024-05-19T11:20:22.282589Z","shell.execute_reply.started":"2024-05-19T11:20:05.44975Z","shell.execute_reply":"2024-05-19T11:20:22.280856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling\nNo need for scaling for word embeddings.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nbag_of_words = scaler.fit_transform(bag_of_words)\ntf_idf = scaler.fit_transform(tf_idf)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:20:22.284535Z","iopub.execute_input":"2024-05-19T11:20:22.285517Z","iopub.status.idle":"2024-05-19T11:20:24.469951Z","shell.execute_reply.started":"2024-05-19T11:20:22.28546Z","shell.execute_reply":"2024-05-19T11:20:24.468911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA\nSometimes PCA can improve the scores.","metadata":{}},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n\n# pca = PCA(n_components=0.95)\n# bag_of_words_pca = pca.fit_transform(bag_of_words)\n# tf_idf_pca = pca.fit_transform(tf_idf)\n# email_embeddings_pca = pca.fit_transform(email_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:20:24.471375Z","iopub.execute_input":"2024-05-19T11:20:24.471773Z","iopub.status.idle":"2024-05-19T11:20:24.476973Z","shell.execute_reply.started":"2024-05-19T11:20:24.471735Z","shell.execute_reply":"2024-05-19T11:20:24.475919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split to train and test data\nOur dataset is roughly balanced, and it's good for training supervised models. But in real world, the dataset is often very unbalanced. So let's create an unbalanced dataset with 5% of anomalies.","metadata":{}},{"cell_type":"code","source":"def shuffle_data(features, labels):\n    indices = np.arange(features.shape[0])\n    np.random.shuffle(indices)\n    return features[indices], labels[indices]\n\ndef unbalanced_dataset(dataset): \n    labels = np.asarray(data[\"Label\"])\n    \n    normal = dataset[labels == 0]\n    anomalies = dataset[labels == 1]\n\n    # select random 5% of anomalies\n    num_anomalies = int(len(normal) * 0.05)\n    anomaly_indices = np.random.choice(anomalies.shape[0], num_anomalies, replace=False)\n    selected_anomalies = anomalies[anomaly_indices]\n    \n    # combine\n    X_unbalanced = np.vstack([normal, selected_anomalies])\n    y_unbalanced = np.concatenate([np.full((len(normal)), 1), np.full((len(selected_anomalies)), 0)])\n\n    # shuffle\n    X_unbalanced, y_unbalanced = shuffle_data(X_unbalanced, y_unbalanced)\n    return X_unbalanced, y_unbalanced","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:20:24.478304Z","iopub.execute_input":"2024-05-19T11:20:24.478835Z","iopub.status.idle":"2024-05-19T11:20:24.493701Z","shell.execute_reply.started":"2024-05-19T11:20:24.4788Z","shell.execute_reply":"2024-05-19T11:20:24.492361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.asarray(data[\"Label\"])\n\nsets = [\n#     [bag_of_words, labels],\n#     [tf_idf, labels],\n#     [np.array(email_embeddings), labels],\n    list(unbalanced_dataset(bag_of_words)),\n    list(unbalanced_dataset(tf_idf)),\n    list(unbalanced_dataset(np.array(email_embeddings))),\n#     list(unbalanced_dataset(bag_of_words_pca)),\n#     list(unbalanced_dataset(tf_idf_pca)),\n#     list(unbalanced_dataset(email_embeddings_pca)),\n]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:20:24.495573Z","iopub.execute_input":"2024-05-19T11:20:24.496307Z","iopub.status.idle":"2024-05-19T11:20:25.376258Z","shell.execute_reply.started":"2024-05-19T11:20:24.49625Z","shell.execute_reply":"2024-05-19T11:20:25.374812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scores for cross-validation\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n\nscoring = {\n    'accuracy': make_scorer(accuracy_score),\n    'precision': make_scorer(precision_score, average='binary'),\n    'recall': make_scorer(recall_score, average='binary'),\n    'f1_score': make_scorer(f1_score, average='binary')\n}\n\ndef display_results(results):\n    print(\"Test Scores:\")\n    for score_name in results:\n        print(f\"{score_name}: {results[score_name].mean()}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:20:25.377935Z","iopub.execute_input":"2024-05-19T11:20:25.378433Z","iopub.status.idle":"2024-05-19T11:20:25.464769Z","shell.execute_reply.started":"2024-05-19T11:20:25.37838Z","shell.execute_reply":"2024-05-19T11:20:25.46383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification algorithms (supervised learning)\nhttps://towardsdatascience.com/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2 <br />\nAs we can see, the worse results are given by Naive Bayes with CountVectorizer. Other algorithms are dealing more or less well. The best results are given by VotingClassifier. <br />\nWe see that embeddings indeed performs better.\n## Evaluation metrics:\n**Accuracy** = (True Positives + True Negatives) / (True Positives + False Positives + True Negatives + False Negatives) <br />\nAccuracy measures the proportion of correct predictions made by the model out of the total number of predictions.\n\n\n**Precision** = True Positives / (True Positives + False Positives) <br />\nPrecision measures the proportion of true positive predictions out of all the positive predictions made by the model.\n\n\n**Recall** = True Positives / (True Positives + False Negatives) <br />\nRecall measures the proportion of true positive predictions to the number of actual positives (true positives + false negatives).\n\n\n**F1 Score** = 2 * (Precision * Recall) / (Precision + Recall) <br />\nAn F1 score reaches its best value at 1 (perfect precision and recall) and its worst value at 0.","metadata":{}},{"cell_type":"markdown","source":"The algorithms are run three times: on bag of words, TF-IDF and word embeddings\n## Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nGB = GaussianNB()\n\nfor curr_set in sets:\n    X, y = curr_set[0], curr_set[1]\n    \n    results = cross_validate(GB, X, y, cv=5, scoring=scoring, return_train_score = False)\n    display_results(results)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:27:39.210478Z","iopub.execute_input":"2024-05-19T11:27:39.213703Z","iopub.status.idle":"2024-05-19T11:27:45.198684Z","shell.execute_reply.started":"2024-05-19T11:27:39.213622Z","shell.execute_reply":"2024-05-19T11:27:45.197059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# lbfgs is the fastest\nfor solver in ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky']:\n    print(solver)\n    LR = LogisticRegression(max_iter=1000, solver=solver)\n\n    for curr_set in sets:\n        X, y = curr_set[0], curr_set[1]\n\n        results = cross_validate(LR, X, y, cv=5, scoring=scoring, return_train_score = False)\n        display_results(results)\n        \nLR = LogisticRegression(max_iter=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *k*-nearest neighbors","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier()\n\nfor curr_set in sets:\n    X, y = curr_set[0], curr_set[1]\n    \n    results = cross_validate(KNN, X, y, cv=5, scoring=scoring, return_train_score = False)\n    display_results(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear support vector","metadata":{}},{"cell_type":"code","source":"# very long\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MinMaxScaler\n\nSVC = LinearSVC(max_iter = 8000)\n\nfor curr_set in sets:\n    X, y = MinMaxScaler().fit_transform(curr_set[0]), curr_set[1]\n    \n    results = cross_validate(SVC, X, y, cv=5, scoring=scoring, return_train_score = False)\n    display_results(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# 'entropy' is the fastest\nfor criterion in ['gini', 'entropy', 'log_loss']:\n    print(criterion)\n    DTC = DecisionTreeClassifier(criterion=criterion)\n\n    for curr_set in sets:\n        X, y = curr_set[0], curr_set[1]\n\n        results = cross_validate(DTC, X, y, cv=5, scoring=scoring, return_train_score = False)\n        display_results(results)\n        \nDTC = DecisionTreeClassifier(criterion='entropy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# 5 is the fastest, with the same results\nfor n_estimators in [5, 10, 15, 25, 50, 100]:\n    print(n_estimators)\n    RFC = RandomForestClassifier(n_estimators=n_estimators)\n\n    for curr_set in sets:\n        X, y = curr_set[0], curr_set[1]\n\n        results = cross_validate(RFC, X, y, cv=5, scoring=scoring, return_train_score = False)\n        display_results(results)\n        \nRFC = RandomForestClassifier(n_estimators=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Voting classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nVC = VotingClassifier(estimators=[('LR',LR),('RFC',RFC)], voting='hard')\n\nfor curr_set in sets:\n    X, y = curr_set[0], curr_set[1]\n    \n    results = cross_validate(VC, X, y, cv=5, scoring=scoring, return_train_score = False)\n    display_results(results)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsupervised algorithms ","metadata":{}},{"cell_type":"markdown","source":"We can't use cross_validate function for unsupervised algorithms, so let's create 5 different sets manually. For One-class SVM, Local Outlier Factor, K-Means and Gaussian Mixture model, our train sets will consist only from normal data. For Isolation Forest, we will create separate train models with anomalies.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ndef create_cross_validation_sets(dataset, labels):\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n    X_trains, X_tests, y_trains, y_tests = [], [], [], []\n\n    for train_index, test_index in kf.split(dataset):\n        X_trains.append(dataset[train_index])\n        X_tests.append(dataset[test_index])\n        y_trains.append(labels[train_index])\n        y_tests.append(labels[test_index])\n\n    # For algorithms (except Isolation Forest) remove anomalies from train sets\n    X_trains_normal = []\n\n    for X_train, y_train in zip(X_trains, y_trains):\n        X_trains_normal.append(X_train[y_train == 1]) \n        \n    return X_trains_normal, X_trains, X_tests, y_trains, y_tests","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:29:36.488572Z","iopub.execute_input":"2024-05-19T11:29:36.489186Z","iopub.status.idle":"2024-05-19T11:29:36.503221Z","shell.execute_reply.started":"2024-05-19T11:29:36.489131Z","shell.execute_reply":"2024-05-19T11:29:36.501266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score\nfrom scipy.spatial.distance import cdist\nimport time\n\ndef is_balanced(labels):\n    # returns the sorted unique elements of an array\n    values, counts = np.unique(labels, return_counts=True)\n    return counts[1] / counts[0] < 5\n\ndef gaussian_mixture_model_predictions(model, X_test, y_test):\n    scores = model.score_samples(X_test)\n    threshold = np.percentile(scores, 50 if is_balanced(y_test) else 5)\n    return np.array([0 if score < threshold else 1 for score in scores]) \n\ndef kmeans_predictions(model, X_test, y_test):\n    labels = model.predict(X_test)\n    centroids = model.cluster_centers_\n    distances = cdist(X_test, centroids, 'euclidean')\n    min_distances = distances[np.arange(len(distances)), labels]\n    threshold = np.percentile(min_distances, 50 if is_balanced(y_test) else 5)\n    return np.array([0 if d > threshold else 1 for d in min_distances])\n\ndef train_test_model(model, current_set, labels, \n                     novelty=True, gaussian_mixture_model=False, kmeans=False):\n    \n    X_trains_normal, X_trains, X_tests, y_trains, y_tests = create_cross_validation_sets(current_set, labels)\n    \n    fit_time_total, accuracy_total, precision_total, recall_total, f1score_total = 0, 0, 0, 0, 0\n    pred = []\n\n    for X_train, X_test, y_test in zip(\n        X_trains_normal if novelty else X_trains, \n        X_tests, \n        y_tests if novelty else y_trains):\n        \n        if novelty: \n            start_time = time.time()\n            model.fit(X_train)\n            fit_time_total += time.time() - start_time\n\n            if gaussian_mixture_model:\n                pred = gaussian_mixture_model_predictions(model, X_test, y_test) \n            elif kmeans:\n                pred = kmeans_predictions(model, X_test, y_test)\n            else:\n                pred = np.array([1 if p == -1 else 0 for p in model.predict(X_test)])\n        \n        else:\n            start_time = time.time()\n            pred = np.array([1 if p == -1 else 0 for p in model.fit_predict(X_train)])\n            fit_time_total += time.time() - start_time\n\n        accuracy_total += balanced_accuracy_score(y_test, pred)\n        precision_total += precision_score(y_test, pred)\n        recall_total += recall_score(y_test, pred)\n        f1score_total += f1_score(y_test, pred)\n\n    print(f\"fit_time: {fit_time_total / len(X_trains_normal)}\")\n    print(f\"accuracy: {accuracy_total / len(X_trains_normal)}\")\n    print(f\"precision: {precision_total / len(X_trains_normal)}\")\n    print(f\"recall: {recall_total / len(X_trains_normal)}\")\n    print(f\"f1-score: {f1score_total / len(X_trains_normal)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:29:40.014705Z","iopub.execute_input":"2024-05-19T11:29:40.015127Z","iopub.status.idle":"2024-05-19T11:29:40.036824Z","shell.execute_reply.started":"2024-05-19T11:29:40.015092Z","shell.execute_reply":"2024-05-19T11:29:40.034709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-class SVM\nEach entry is either 1 for normal data or -1 for an anomaly. <br/>\nOne-class SVM is trained on the normal data only, and tested on test set.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import OneClassSVM\n\nfor kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n    for nu in [0.2, 0.5, 0.7]:\n        print(kernel, nu)\n        model = OneClassSVM(kernel = kernel, nu = nu)\n\n        for curr_set in sets:\n            X, y = curr_set[0], curr_set[1]\n            train_test_model(model, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Local Outlier Factor\nLocal outlier factor can be used in two ways: build a new model for the whole dataset each time, or fit it on the normal data only, and predict it on different sets.","metadata":{}},{"cell_type":"code","source":"# outlier detection\nfrom sklearn.neighbors import LocalOutlierFactor\n\nfor n_neighbors in [3, 5, 10, 15, 20, 25]:\n    print(n_neighbors)\n    model = LocalOutlierFactor(n_neighbors=n_neighbors)\n\n    for curr_set in sets:\n        X, y = curr_set[0], curr_set[1]\n        train_test_model(model, X, y, novelty=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T18:58:45.335054Z","iopub.execute_input":"2024-05-08T18:58:45.336396Z","iopub.status.idle":"2024-05-08T19:01:45.104207Z","shell.execute_reply.started":"2024-05-08T18:58:45.336338Z","shell.execute_reply":"2024-05-08T19:01:45.102737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# novelty detection\nfor n_neighbors in [3, 5, 10, 15, 20, 25]:\n    print(n_neighbors)\n    model = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True)\n\n    for curr_set in sets:\n        X, y = curr_set[0], curr_set[1]\n        train_test_model(model, X, y)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T19:01:45.107382Z","iopub.execute_input":"2024-05-08T19:01:45.107811Z","iopub.status.idle":"2024-05-08T19:05:50.571329Z","shell.execute_reply.started":"2024-05-08T19:01:45.107775Z","shell.execute_reply":"2024-05-08T19:05:50.569537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Isolation Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nmodel = IsolationForest(n_estimators=30, max_samples=32)\n\nfor curr_set in sets:\n    X, y = curr_set[0], curr_set[1]\n    train_test_model(model, X, y, novelty=False)\n    \n# sometimes, the precision, recall and f1-score are 0. It probably means that the model predicted everything as normal","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian Mixture Model\nhttps://vitalflux.com/gaussian-mixture-models-what-are-they-when-to-use/ <br/>\nhttps://towardsdatascience.com/gaussian-mixture-model-clusterization-how-to-select-the-number-of-components-clusters-553bef45f6e4 (also determining appropriate number of clusters)<br/>\nhttps://github.com/vlavorini/ClusterCardinality/blob/master/Cluster%20Cardinality.ipynb<br/>\nhttps://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms<br/>\nhttps://stats.stackexchange.com/questions/349258/correct-number-of-components-in-gmm-according-to-bic-and-aic-plots<br/>\nhttps://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm<br/>\nhttps://grabngoinfo.com/how-to-decide-the-number-of-clusters-data-science-interview-questions-and-answers/\nBayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) scores","metadata":{}},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\nnormal_dataset = np.array(email_embeddings)[labels == 0]\n\nn_clusters = range(1, 15)\nbics = []\nfor k in n_clusters:\n    gmm = GaussianMixture(n_components=k, random_state=42)\n    gmm.fit(normal_dataset)\n    bics.append(gmm.bic(normal_dataset))\n\nplt.plot(n_clusters, bics, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('BIC Score')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\nmodel = GaussianMixture(n_components=4, covariance_type='spherical', random_state=42)\n\nfor curr_set in sets:\n    X, y = curr_set[0], curr_set[1]\n    train_test_model(model, X, y, gaussian_mixture_model=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Means\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py <br />\nhttps://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/ <br />\nhttps://stackoverflow.com/a/69024239 <br/>\nhttps://stackoverflow.com/a/27586132 <br />\nhttps://www.dataknowsall.com/textclustering.html <br />\nhttps://www.kaggle.com/code/naren3256/kmeans-clustering-and-cluster-visualization-in-3d/notebook <br />\nhttps://medium.com/@jwbtmf/visualizing-data-using-k-means-clustering-unsupervised-machine-learning-8b59eabfcd3d <br />\nFor choosing the appropriate number of clusters, we can use Elbow method or Silhouette score. Let's use Elbow method.","metadata":{}},{"cell_type":"code","source":"# finding the optimal number of clusters using an elbow method\nfrom sklearn.cluster import KMeans\n\nn_clusters = range(1, 15)\ninertias = []\nfor k in n_clusters:\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(normal_dataset)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(n_clusters, inertias, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.xticks(n_clusters)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\nmodel = KMeans(n_clusters=9)\n\nfor curr_set in sets:\n    X, y = curr_set[0], curr_set[1]\n    train_test_model(model, X, y, kmeans=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for curr_set in sets:\n#     curr_x_train, curr_y_train, curr_x_test, curr_y_test = curr_set[0], curr_set[1], curr_set[2], curr_set[3]\n\n#     # train only on normal data\n#     x_train_normal = curr_x_train[curr_y_train == 0]\n    \n#     labels = kmeans.fit(x_train_normal)\n\n#     # anomalies are far from the clusters' centers\n#     labels = kmeans.predict(curr_x_train)\n#     centroids = kmeans.cluster_centers_\n#     distances = cdist(curr_x_train, centroids, 'euclidean')\n#     min_distances = distances[np.arange(len(distances)), labels]\n#     threshold = np.percentile(min_distances, 50)\n#     pred1 = np.array([1 if d > threshold else 0 for d in min_distances])\n\n#     labels = kmeans.predict(curr_x_test)\n#     centroids = kmeans.cluster_centers_\n#     distances = cdist(curr_x_test, centroids, 'euclidean')\n#     min_distances = distances[np.arange(len(distances)), labels]\n#     pred2 = np.array([1 if d > threshold else 0 for d in min_distances])\n\n#     print_stats(curr_y_train, pred1, curr_y_test, pred2, \"KMeans\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep learning (unsupervised)\n(subset of machine learning algorithms)<br/>\n[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\ndef print_stats(predictions, X, y, title):\n    # Generate the classification report\n    report = classification_report(y, predictions, target_names=['Ham', 'Spam'])\n    print(report)\n\n    # Generate the confusion matrix\n    conf = confusion_matrix(y, predictions)\n    plt.title(title)\n    ax= plt.subplot()\n    sns.heatmap(conf, annot=True, fmt=\"\", linewidths=2, cmap=\"Greens\")\n    ax.set_xlabel('Predicted');\n    ax.set_ylabel('Real');\n    ax.xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax.yaxis.set_ticklabels(['Ham', 'Spam']);\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Autoencoder\nhttps://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras import Sequential, Model\n\nX, y = sets[2][0], sets[2][1]\n\nX_trains_normal, _, X_tests, _, y_tests = create_cross_validation_sets(X, y)\nX_train_normal, X_test, y_test = X_trains_normal[0], X_tests[0], y_tests[0]\n\n# Input layer\ninput = Input(shape=(X_train_normal.shape[1],))\n\n# Encoder layers\nencoder = Sequential([\n    Dense(256, activation='relu'),  \n    Dense(64, activation='relu'),  \n    Dense(16, activation='relu'),\n    Dense(4, activation='relu')])(input)\n\n# Decoder layers\ndecoder = Sequential([\n    Dense(16, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(X_train_normal.shape[1], activation=\"sigmoid\")])(encoder)\n\n# Create the autoencoder\nautoencoder = Model(inputs=input, outputs=decoder)\nautoencoder.compile(optimizer='adam', loss='mse')\nhistory = autoencoder.fit(X_train_normal, X_train_normal, \n                          epochs=40, \n                          batch_size=256,\n                          validation_split=0.2,\n                          shuffle=True)\n\nreconstructed_test = autoencoder.predict(X_test)\n\n# Calculate the mean squared error of the reconstruction\nmse_test = np.mean(np.power(X_test - reconstructed_test, 2), axis=1)\n\n# Determine a threshold for classifying a point as an anomaly\nthreshold = np.percentile(mse_test, 95)  # Set threshold at the 95th percentile\n\n# Anything above the threshold is considered an anomaly\npred = (mse_test > threshold).astype(int)\n\nprint(f\"accuracy: {balanced_accuracy_score(y_test, pred)}\")\nprint(f\"precision: {precision_score(y_test, pred)}\")\nprint(f\"recall: {recall_score(y_test, pred)}\")\nprint(f\"f1-score: {f1_score(y_test, pred)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T11:56:52.60617Z","iopub.execute_input":"2024-05-19T11:56:52.60659Z","iopub.status.idle":"2024-05-19T11:57:01.829698Z","shell.execute_reply.started":"2024-05-19T11:56:52.606553Z","shell.execute_reply":"2024-05-19T11:57:01.828155Z"},"trusted":true},"execution_count":null,"outputs":[]}]}