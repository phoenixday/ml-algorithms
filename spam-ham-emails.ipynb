{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1603933,"sourceType":"datasetVersion","datasetId":946753},{"sourceId":5260904,"sourceType":"datasetVersion","datasetId":3061553},{"sourceId":6420219,"sourceType":"datasetVersion","datasetId":3703513}],"dockerImageVersionId":30407,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kapusharinka/spam-detection?scriptVersionId=171492618\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport time\nfrom pprint import pprint\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T05:39:00.427134Z","iopub.execute_input":"2024-04-03T05:39:00.427633Z","iopub.status.idle":"2024-04-03T05:39:00.457183Z","shell.execute_reply.started":"2024-04-03T05:39:00.427588Z","shell.execute_reply":"2024-04-03T05:39:00.456179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Articles about ML\nhttps://github.com/christianversloot/machine-learning-articles/tree/main","metadata":{}},{"cell_type":"markdown","source":"# Data preprocessing\n## Preparing datasets","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/email-spam-dataset/enronSpamSubset.csv')\ndata.info()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:00.458761Z","iopub.execute_input":"2024-04-03T05:39:00.45959Z","iopub.status.idle":"2024-04-03T05:39:00.910737Z","shell.execute_reply.started":"2024-04-03T05:39:00.45955Z","shell.execute_reply":"2024-04-03T05:39:00.909633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete unneeded columns\ndata.drop([\"Unnamed: 0\",\"Unnamed: 0.1\"],inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:00.912218Z","iopub.execute_input":"2024-04-03T05:39:00.912699Z","iopub.status.idle":"2024-04-03T05:39:00.920481Z","shell.execute_reply.started":"2024-04-03T05:39:00.912658Z","shell.execute_reply":"2024-04-03T05:39:00.919426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove emails with body longer than 10000\ndata = data[data['Body'].apply(lambda x: len(str(x)) <= 10000)]","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:00.923609Z","iopub.execute_input":"2024-04-03T05:39:00.924024Z","iopub.status.idle":"2024-04-03T05:39:00.939262Z","shell.execute_reply.started":"2024-04-03T05:39:00.923977Z","shell.execute_reply":"2024-04-03T05:39:00.937905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a fraction\n# data = data.sample(frac=0.7)\n# remove missing values (NaN)\ndata.dropna(inplace=True)\ndata.info()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:00.940811Z","iopub.execute_input":"2024-04-03T05:39:00.941187Z","iopub.status.idle":"2024-04-03T05:39:00.967746Z","shell.execute_reply.started":"2024-04-03T05:39:00.941151Z","shell.execute_reply":"2024-04-03T05:39:00.966559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nax = sns.countplot(x=data[\"Label\"], palette=(\"Pastel2\"))\nabs_values = data[\"Label\"].value_counts(ascending=False).values\nax.bar_label(container=ax.containers[0], labels=abs_values)\nax.set_xticklabels(['hams', 'spams'])\n\nplt.xlabel(None)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:00.969456Z","iopub.execute_input":"2024-04-03T05:39:00.970246Z","iopub.status.idle":"2024-04-03T05:39:01.843816Z","shell.execute_reply.started":"2024-04-03T05:39:00.970204Z","shell.execute_reply":"2024-04-03T05:39:01.84234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emails = data[\"Body\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:01.845534Z","iopub.execute_input":"2024-04-03T05:39:01.846554Z","iopub.status.idle":"2024-04-03T05:39:01.852972Z","shell.execute_reply.started":"2024-04-03T05:39:01.846479Z","shell.execute_reply":"2024-04-03T05:39:01.85092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I'm removing unneeded characters, like HTML tags, emails etc.","metadata":{}},{"cell_type":"code","source":"import re\n\n# remove emails\nemails = [re.sub('\\S*@\\S*\\s?', '', text) for text in emails]\n# remove url links\nemails = [re.sub('\\S*(http[s]?://|www\\.)\\S*', '', text) for text in emails]\n# remove HTML tags\nemails = [re.sub(r\"'<.*?>'\", \"\", text) for text in emails]\n# remove special characters and numbers\nemails = [re.sub(\"[^a-zA-Z]\",\" \",text) for text in emails]\n# remove too short (2- characters) words\nemails = [re.sub(r\"\\b\\w{1,2}\\b\", \"\",text) for text in emails]\n# and too long (17+ characters) \nemails = [re.sub(r\"\\b\\w{17,}\\b\", \"\",text) for text in emails]\n# remove multiple spaces\nemails = [re.sub(' +', ' ', text).strip() for text in emails]\n# lower\nemails = [text.lower() for text in emails]\n\nemails[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:01.854925Z","iopub.execute_input":"2024-04-03T05:39:01.855879Z","iopub.status.idle":"2024-04-03T05:39:06.178945Z","shell.execute_reply.started":"2024-04-03T05:39:01.855825Z","shell.execute_reply":"2024-04-03T05:39:06.177585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization and lemmatization\nTokenization: [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) (page 180)\n<ul>\n    <li>one-hot encoding of tokens</li>\n    <li>token embedding</li>\n</ul>\nFor more, look at <i>Deep Learning</i> section below.","metadata":{}},{"cell_type":"markdown","source":"We can choose between stemming or lemmatization - lemmatizators are slower, but change tenses and nouns. <br/>\nhttps://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n\nFirstly I used NLTK lemmatization, but it is very slow on my computer, so I tried SpaCy instead (https://spacy.io).","metadata":{}},{"cell_type":"code","source":"custom_stopwords = ['subject', 'empty', 'email', 'mail', 'enron', 'linux', 'list', 'get', 'http', 'vince', 'com', 'org', 'www', 'etc', 'ect', 'edu', 'hou', 'would', 'need']","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:06.18049Z","iopub.execute_input":"2024-04-03T05:39:06.180993Z","iopub.status.idle":"2024-04-03T05:39:06.187655Z","shell.execute_reply.started":"2024-04-03T05:39:06.180944Z","shell.execute_reply":"2024-04-03T05:39:06.186247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using SpaCy\nhttps://stackoverflow.com/a/75215495","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# add stop words\nfor word in custom_stopwords:\n    nlp.vocab[word].is_stop = True","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:06.192871Z","iopub.execute_input":"2024-04-03T05:39:06.19332Z","iopub.status.idle":"2024-04-03T05:39:22.481148Z","shell.execute_reply.started":"2024-04-03T05:39:06.193278Z","shell.execute_reply":"2024-04-03T05:39:22.480048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatization\nemails = [[token.lemma_ for token in nlp(text) if not token.is_stop] for text in emails]\nemails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:39:22.48286Z","iopub.execute_input":"2024-04-03T05:39:22.483805Z","iopub.status.idle":"2024-04-03T05:43:37.914009Z","shell.execute_reply.started":"2024-04-03T05:39:22.483742Z","shell.execute_reply":"2024-04-03T05:43:37.912714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature extraction \nCreating a vector of features (words) for each email. <br />\nOpenAI:\n> Both CountVectorizer and TF-IDF (Term Frequency-Inverse Document Frequency) from scikit-learn are popular techniques for feature extraction in text data like emails, and each has its own merits.\n> \n> CountVectorizer creates a Bag of Words (BoW) model, where the features are the counts of each word in the document. This method is simple and easy to implement but can give more importance to words that appear frequently, regardless of their significance in distinguishing spam from non-spam emails.\n> \n> TF-IDF, on the other hand, takes into account not only the frequency of a word in a document but also its inverse frequency across all documents. This means that words that are common across all emails will receive lower weights, while words that are unique to specific emails will receive higher weights. This can be advantageous for spam detection, as spam emails often contain specific words or phrases that are less common in legitimate emails.\n> \n> In general, TF-IDF tends to work better than CountVectorizer for spam detection because it can better capture the importance of different words. However, the choice between the two methods will depend on the specific characteristics of the dataset and the problem you're trying to solve. It's a good idea to experiment with both techniques and evaluate their performance on your dataset using cross-validation or a separate validation set. This will help you determine which method works best for your particular spam detection task.","metadata":{}},{"cell_type":"code","source":"# bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# ngram_range=(1,2) means that the model will consider bigrams too\n# min_df=0.003 means that the model will not consider rare words\ncount_vectorizer = CountVectorizer(max_features=25000, ngram_range=(1,3), min_df=0.003, max_df=0.9)\nbag_of_words = count_vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(bag_of_words.shape)\ncount_vectorizer.get_feature_names_out()[:10] # first 10 in alphabetical order","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:43:37.915423Z","iopub.execute_input":"2024-04-03T05:43:37.915806Z","iopub.status.idle":"2024-04-03T05:43:46.056348Z","shell.execute_reply.started":"2024-04-03T05:43:37.915771Z","shell.execute_reply":"2024-04-03T05:43:46.054946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_vectorizer = TfidfVectorizer(max_features=25000, ngram_range=(1,3), min_df=0.003, max_df=0.9)\ntf_idf = tf_vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(tf_idf.shape)\ntf_vectorizer.get_feature_names_out()[:10]","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:43:46.058589Z","iopub.execute_input":"2024-04-03T05:43:46.059073Z","iopub.status.idle":"2024-04-03T05:43:54.357638Z","shell.execute_reply.started":"2024-04-03T05:43:46.05902Z","shell.execute_reply":"2024-04-03T05:43:54.356297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word embeddings\nA word embedding is a vector, that tries to capture \"meaning\" of a word/sentence/document etc... Some of the algorithms are provided by Word2Vec, GloVe. I used one provided by SpaCy.","metadata":{}},{"cell_type":"markdown","source":"### Using SpaCy\nAlso check [sense2vec](http://https://github.com/explosion/sense2vec)","metadata":{}},{"cell_type":"code","source":"nlp = spacy.blank(\"en\")\nnlp.from_disk('/kaggle/input/en-core-web-lg/en_core_web_lg/en_core_web_lg-3.6.0')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:43:54.359235Z","iopub.execute_input":"2024-04-03T05:43:54.360524Z","iopub.status.idle":"2024-04-03T05:44:01.016106Z","shell.execute_reply.started":"2024-04-03T05:43:54.360463Z","shell.execute_reply":"2024-04-03T05:44:01.014839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating vectors for each email. We take a vector for each word from email, and finding the average vector for a whole email.","metadata":{}},{"cell_type":"code","source":"email_embeddings = []\nfor email in emails:\n    if len(email) > 0:\n        spacy_doc = nlp(' '.join(email))\n        avg_vector = sum([token.vector for token in spacy_doc]) / len(email)\n    else:\n        avg_vector = nlp('').vector\n    email_embeddings.append(avg_vector)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:44:01.017548Z","iopub.execute_input":"2024-04-03T05:44:01.017905Z","iopub.status.idle":"2024-04-03T05:44:08.68256Z","shell.execute_reply.started":"2024-04-03T05:44:01.017871Z","shell.execute_reply":"2024-04-03T05:44:08.681154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More playing.","metadata":{}},{"cell_type":"markdown","source":"## Word cloud\nHere are word clouds for spams and hams with the most frequent words, created with TF-IDF vectorizer.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\ndata['Tokens'] = emails\n\n# spams\nspams = data.loc[data['Label'] == 1, ['Tokens']]\nspam_x = tf_vectorizer.fit_transform([\" \".join(text) for text in spams['Tokens']]).toarray()\n\ndf = pd.DataFrame(spam_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\ndf.head(10)\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\nplt.title('Spams', fontsize = 40)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:44:08.684262Z","iopub.execute_input":"2024-04-03T05:44:08.684753Z","iopub.status.idle":"2024-04-03T05:44:26.656562Z","shell.execute_reply.started":"2024-04-03T05:44:08.684713Z","shell.execute_reply":"2024-04-03T05:44:26.653344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hams\nhams = data.loc[data['Label'] == 0, ['Tokens']]\nham_x = tf_vectorizer.fit_transform([\" \".join(text) for text in hams['Tokens']]).toarray()\n\ndf = pd.DataFrame(ham_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\nplt.title('Hams', fontsize = 40)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:44:26.658524Z","iopub.execute_input":"2024-04-03T05:44:26.659129Z","iopub.status.idle":"2024-04-03T05:44:37.89958Z","shell.execute_reply.started":"2024-04-03T05:44:26.659075Z","shell.execute_reply":"2024-04-03T05:44:37.897894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split to train and test data\nThe split is needed for supervised algorithms and supervised deep learning. I'm going to expriment with supervised algorithms, trying each on CounVectorizer and on TF-IDF.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split to train and test data for CountVectorizer\nx_train,x_test,y_train,y_test = train_test_split(bag_of_words, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)\n\n# split to train and test data for TF-IDF\nx2_train,x2_test,y2_train,y2_test = train_test_split(tf_idf, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)\n\n# split to train and test data for embeddings\nx3_train,x3_test,y3_train,y3_test = train_test_split(email_embeddings, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:44:37.901851Z","iopub.execute_input":"2024-04-03T05:44:37.902597Z","iopub.status.idle":"2024-04-03T05:44:38.323842Z","shell.execute_reply.started":"2024-04-03T05:44:37.902539Z","shell.execute_reply":"2024-04-03T05:44:38.322414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification algorithms (supervised learning)\nhttps://towardsdatascience.com/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2 <br />\nAs we can see, the worse results are given by Naive Bayes with CountVectorizer. Other algorithms are dealing more or less well. The best results are given by VotingClassifier. <br />\nWe see that embeddings indeed performs better.\n## Evaluation metrics:\n**Accuracy** = (True Positives + True Negatives) / (True Positives + False Positives + True Negatives + False Negatives) <br />\nAccuracy measures the proportion of correct predictions made by the model out of the total number of predictions.\n\n\n**Precision** = True Positives / (True Positives + False Positives) <br />\nPrecision measures the proportion of true positive predictions out of all the positive predictions made by the model.\n\n\n**Recall** = True Positives / (True Positives + False Negatives) <br />\nRecall measures the proportion of true positive predictions to the number of actual positives (true positives + false negatives).\n\n\n**F1 Score** = 2 * (Precision * Recall) / (Precision + Recall) <br />\nAn F1 score reaches its best value at 1 (perfect precision and recall) and its worst value at 0.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, balanced_accuracy_score, precision_score, recall_score, f1_score\n\n# printing statistics\ndef print_stats(y1, pred1, y2, pred2, title):\n    \n    cm1 = confusion_matrix(y1, pred1)\n    cm2 = confusion_matrix(y2, pred2)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n    fig.suptitle(title)\n    \n    sns.heatmap(cm1, annot=True, fmt=\"d\", linewidths=2, cmap=\"Greens\", cbar=False, ax=ax[0])\n    ax[0].set_title('Train set')\n    ax[0].set_xlabel('Predicted');\n    ax[0].set_ylabel('Real');\n    ax[0].xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax[0].yaxis.set_ticklabels(['Ham', 'Spam']);\n    \n    sns.heatmap(cm2, annot=True, fmt=\"d\", linewidths=2, cmap=\"Blues\", cbar=False, ax=ax[1])\n    ax[1].set_title('Test set')\n    ax[1].set_xlabel('Predicted');\n    ax[1].set_ylabel('Real');\n    ax[1].xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax[1].yaxis.set_ticklabels(['Ham', 'Spam']);\n    plt.show()\n    \n    data = {\n        'Accuracy': [\n            \"{:.2f}\".format(balanced_accuracy_score(y1, pred1)), \n            \"{:.2f}\".format(balanced_accuracy_score(y2, pred2))],  \n        'Precision': [\n            \"{:.2f}\".format(precision_score(y1, pred1)), \n            \"{:.2f}\".format(precision_score(y2, pred2))], \n        'Recall': [\n            \"{:.2f}\".format(recall_score(y1, pred1)), \n            \"{:.2f}\".format(recall_score(y2, pred2))],\n        'F1 score': [\n            \"{:.2f}\".format(f1_score(y1, pred1)), \n            \"{:.2f}\".format(f1_score(y2, pred2))] \n    }\n    df = pd.DataFrame(data)\n    df.index = ['Train', 'Test']\n    print(df)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:11:56.49222Z","iopub.execute_input":"2024-04-03T06:11:56.492838Z","iopub.status.idle":"2024-04-03T06:11:56.512829Z","shell.execute_reply.started":"2024-04-03T06:11:56.492772Z","shell.execute_reply":"2024-04-03T06:11:56.511526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The algorithms are run three times: on bag of words, TF-IDF and word embeddings\n## Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nNB = GaussianNB()\nNB.fit(x_train,y_train)\npred1 = NB.predict(x_train)\npred2 = NB.predict(x_test)\nprint_stats(y_train, pred1, y_test, pred2, \"Gaussian Naive Bayes (bag of words)\")\n\nNB2 = GaussianNB()\nNB2.fit(x2_train,y2_train)\npred1 = NB2.predict(x2_train)\npred2 = NB2.predict(x2_test)\nprint_stats(y2_train, pred1, y2_test, pred2, \"Gaussian Naive Bayes (TF-IDF)\")\n\nNB3 = GaussianNB()\nNB3.fit(x3_train,y3_train)\npred1 = NB3.predict(x3_train)\npred2 = NB3.predict(x3_test)\nprint_stats(y3_train, pred1, y3_test, pred2, \"Gaussian Naive Bayes (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:44:38.342617Z","iopub.execute_input":"2024-04-03T05:44:38.343096Z","iopub.status.idle":"2024-04-03T05:44:42.335597Z","shell.execute_reply.started":"2024-04-03T05:44:38.343044Z","shell.execute_reply":"2024-04-03T05:44:42.334589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(x_train,y_train)\npred1 = LR.predict(x_train)\npred2 = LR.predict(x_test)\nprint_stats(y_train, pred1, y_test, pred2, \"Logistic Regression (bag of words)\")\n\nLR2 = LogisticRegression()\nLR2.fit(x2_train,y2_train)\npred1 = LR2.predict(x2_train)\npred2 = LR2.predict(x2_test)\nprint_stats(y2_train, pred1, y2_test, pred2, \"Logistic Regression (TF-IDF)\")\n\nLR3 = LogisticRegression()\nLR3.fit(x3_train,y3_train)\npred1 = LR3.predict(x3_train)\npred2 = LR3.predict(x3_test)\nprint_stats(y3_train, pred1, y3_test, pred2, \"Logistic Regression (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:44:42.337002Z","iopub.execute_input":"2024-04-03T05:44:42.338133Z","iopub.status.idle":"2024-04-03T05:44:47.732391Z","shell.execute_reply.started":"2024-04-03T05:44:42.33809Z","shell.execute_reply":"2024-04-03T05:44:47.731163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *k*-nearest neighbors","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier()\nKNN.fit(x_train,y_train)\npred1 = KNN.predict(x_train)\npred2 = KNN.predict(x_test)\nprint_stats(y_train, pred1, y_test, pred2, \"K Neighbors Classifier (bag of words)\")\n\nKNN2 = KNeighborsClassifier()\nKNN2.fit(x2_train,y2_train)\npred1 = KNN2.predict(x2_train)\npred2 = KNN2.predict(x2_test)\nprint_stats(y2_train, pred1, y2_test, pred2, \"K Neighbors Classifier (TF-IDF)\")\n\nKNN3 = KNeighborsClassifier()\nKNN3.fit(x3_train,y3_train)\npred1 = KNN3.predict(x3_train)\npred2 = KNN3.predict(x3_test)\nprint_stats(y3_train, pred1, y3_test, pred2, \"K Neighbors Classifier (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:44:47.73408Z","iopub.execute_input":"2024-04-03T05:44:47.734477Z","iopub.status.idle":"2024-04-03T05:45:16.367921Z","shell.execute_reply.started":"2024-04-03T05:44:47.734441Z","shell.execute_reply":"2024-04-03T05:45:16.365748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear support vector","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nSVC = LinearSVC(max_iter=2000)\nSVC.fit(x_train,y_train)\npred1 = SVC.predict(x_train)\npred2 = SVC.predict(x_test)\nprint_stats(y_train, pred1, y_test, pred2, \"Linear Support Vector Classifier (bag of words)\")\n\nSVC2 = LinearSVC(max_iter=2000)\nSVC2.fit(x2_train,y2_train)\npred1 = SVC2.predict(x2_train)\npred2 = SVC2.predict(x2_test)\nprint_stats(y2_train, pred1, y2_test, pred2, \"Linear Support Vector Classifier (TF-IDF)\")\n\nSVC3 = LinearSVC(max_iter=2000)\nSVC3.fit(x3_train,y3_train)\npred1 = SVC3.predict(x3_train)\npred2 = SVC3.predict(x3_test)\nprint_stats(y3_train, pred1, y3_test, pred2, \"Linear Support Vector Classifier (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:45:16.370174Z","iopub.execute_input":"2024-04-03T05:45:16.370894Z","iopub.status.idle":"2024-04-03T05:45:25.466213Z","shell.execute_reply.started":"2024-04-03T05:45:16.37083Z","shell.execute_reply":"2024-04-03T05:45:25.464728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nDTC = DecisionTreeClassifier()\nDTC.fit(x_train,y_train)\npred1 = DTC.predict(x_train)\npred2 = DTC.predict(x_test)\nprint_stats(y_train, pred1, y_test, pred2, \"Decision Tree Classifier (bag of words)\")\n\nDTC2 = DecisionTreeClassifier()\nDTC2.fit(x2_train,y2_train)\npred1 = DTC2.predict(x2_train)\npred2 = DTC2.predict(x2_test)\nprint_stats(y2_train, pred1, y2_test, pred2, \"Decision Tree Classifier (TF-IDF)\")\n\nDTC3 = DecisionTreeClassifier()\nDTC3.fit(x3_train,y3_train)\npred1 = DTC3.predict(x3_train)\npred2 = DTC3.predict(x3_test)\nprint_stats(y3_train, pred1, y3_test, pred2, \"Decision Tree Classifier (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:45:25.468194Z","iopub.execute_input":"2024-04-03T05:45:25.468619Z","iopub.status.idle":"2024-04-03T05:46:41.163326Z","shell.execute_reply.started":"2024-04-03T05:45:25.468579Z","shell.execute_reply":"2024-04-03T05:46:41.160842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nRFC.fit(x_train,y_train)\npred1 = RFC.predict(x_train)\npred2 = RFC.predict(x_test)\nprint_stats(y_train, pred1, y_test, pred2, \"Random Forest Classifier (bag of words)\")\n\nRFC2 = RandomForestClassifier()\nRFC2.fit(x2_train,y2_train)\npred1 = RFC2.predict(x2_train)\npred2 = RFC2.predict(x2_test)\nprint_stats(y2_train, pred1, y2_test, pred2, \"Random Forest Classifier (TF-IDF)\")\n\nRFC3 = RandomForestClassifier()\nRFC3.fit(x3_train,y3_train)\npred1 = RFC3.predict(x3_train)\npred2 = RFC3.predict(x3_test)\nprint_stats(y3_train, pred1, y3_test, pred2, \"Random Forest Classifier (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:46:41.1651Z","iopub.execute_input":"2024-04-03T05:46:41.166405Z","iopub.status.idle":"2024-04-03T05:47:31.863087Z","shell.execute_reply.started":"2024-04-03T05:46:41.166351Z","shell.execute_reply":"2024-04-03T05:47:31.861786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Voting classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nVC = VotingClassifier(estimators=[('LR',LR),('KNN',KNN),('DTC',DTC),('RFC',RFC),('SVC',SVC)], voting='hard')\nVC.fit(x_train,y_train)\npred1 = VC.predict(x_train)\npred2 = VC.predict(x_test)\nprint_stats(y_train, pred1, y_test, pred2, \"Voting Classifier (bag of words)\")\n\nVC2 = VotingClassifier(estimators=[('LR',LR2),('KNN',KNN2),('DTC',DTC2),('RFC',RFC2),('SVC',SVC2)], voting='hard')\nVC2.fit(x2_train,y2_train)\npred1 = VC2.predict(x2_train)\npred2 = VC2.predict(x2_test)\nprint_stats(y2_train, pred1, y2_test, pred2, \"Voting Classifier (TF-IDF)\")\n\nVC3 = VotingClassifier(estimators=[('LR',LR3),('KNN',KNN3),('DTC',DTC3),('RFC',RFC3),('SVC',SVC3)], voting='hard')\nVC3.fit(x3_train,y3_train)\npred1 = VC3.predict(x3_train)\npred2 = VC3.predict(x3_test)\nprint_stats(y3_train, pred1, y3_test, pred2, \"Voting Classifier (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:47:31.864403Z","iopub.execute_input":"2024-04-03T05:47:31.864801Z","iopub.status.idle":"2024-04-03T05:50:14.793798Z","shell.execute_reply.started":"2024-04-03T05:47:31.864762Z","shell.execute_reply":"2024-04-03T05:50:14.792474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsupervised algorithms ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, balanced_accuracy_score, precision_score, recall_score, f1_score\n\n# printing statistics\ndef print_single_stats(y, pred, title):\n    conf = confusion_matrix(y_pred=pred,y_true=y)\n    plt.title(title)\n    ax=plt.subplot()\n    sns.heatmap(conf, annot=True, fmt=\"d\", linewidths=2, cmap=\"Greens\", cbar=False)\n    ax.set_xlabel('Predicted');\n    ax.set_ylabel('Real');\n    ax.xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax.yaxis.set_ticklabels(['Ham', 'Spam']);\n    plt.show()\n    \n    data = {\n        'Accuracy': [\n            \"{:.2f}\".format(balanced_accuracy_score(y, pred))],  \n        'Precision': [\n            \"{:.2f}\".format(precision_score(y, pred))], \n        'Recall': [\n            \"{:.2f}\".format(recall_score(y, pred))],\n        'F1 score': [\n            \"{:.2f}\".format(f1_score(y, pred))] \n    }\n    df = pd.DataFrame(data)\n    df.index = ['Test']\n    print(df)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:50:14.795976Z","iopub.execute_input":"2024-04-03T05:50:14.796461Z","iopub.status.idle":"2024-04-03T05:50:14.807734Z","shell.execute_reply.started":"2024-04-03T05:50:14.796409Z","shell.execute_reply":"2024-04-03T05:50:14.80648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA\nSometimes PCA can improve the scores.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ntf_idf_pca = pca.fit_transform(tf_idf)\n\nx2_train_pca,x2_test_pca,y2_train_pca,y2_test_pca = train_test_split(tf_idf_pca, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:08:37.077875Z","iopub.execute_input":"2024-04-03T06:08:37.07834Z","iopub.status.idle":"2024-04-03T06:08:41.986555Z","shell.execute_reply.started":"2024-04-03T06:08:37.078286Z","shell.execute_reply":"2024-04-03T06:08:41.985234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-class SVM\nEach entry is either 1 for normal data or -1 for an anomaly. <br/>\nOne-class SVM is trained on the normal data only, and tested on test set.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import OneClassSVM\n\n# train only on normal data\nx_train_normal = x2_train_pca[y2_train == 0]\n\nOCSVM = OneClassSVM(kernel='sigmoid', nu=0.5, gamma='auto')\nOCSVM.fit(x_train_normal)\npred = np.array([1 if p == -1 else 0 for p in OCSVM.predict(x2_test_pca)])\nprint_single_stats(y2_test_pca, pred, \"One-class SVM on the test set\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:46:59.847583Z","iopub.execute_input":"2024-04-03T06:46:59.84806Z","iopub.status.idle":"2024-04-03T06:47:00.991372Z","shell.execute_reply.started":"2024-04-03T06:46:59.848018Z","shell.execute_reply":"2024-04-03T06:47:00.990448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Local Outlier Factor\nLocal outlier factor can be used in two ways: build a new model for the whole dataset each time, or fit it on the normal data only, and predict it on different sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\n\nLOF = LocalOutlierFactor(n_neighbors=10, contamination=0.5)\npred = np.array([1 if p == -1 else 0 for p in LOF.fit_predict(x2_train_pca)])\nprint_single_stats(y2_train_pca, pred, \"Local Outlier Factor\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:45:22.228053Z","iopub.execute_input":"2024-04-03T06:45:22.228545Z","iopub.status.idle":"2024-04-03T06:45:22.439442Z","shell.execute_reply.started":"2024-04-03T06:45:22.228489Z","shell.execute_reply":"2024-04-03T06:45:22.438047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Isolation Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nIF = IsolationForest(n_estimators=30, max_samples=32, contamination=0.50)\nIF.fit(x2_train_pca)\npred1 = np.array([1 if p == -1 else 0 for p in IF.predict(x2_train_pca)])\npred2 = np.array([1 if p == -1 else 0 for p in IF.predict(x2_test_pca)])\nprint_stats(y2_train_pca, pred1, y2_test_pca, pred2, \"Isolation Forest (tf-idf)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:13:04.695794Z","iopub.execute_input":"2024-04-03T06:13:04.696233Z","iopub.status.idle":"2024-04-03T06:13:05.224801Z","shell.execute_reply.started":"2024-04-03T06:13:04.696194Z","shell.execute_reply":"2024-04-03T06:13:05.223679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian Mixture Model\nhttps://vitalflux.com/gaussian-mixture-models-what-are-they-when-to-use/ <br/>\nhttps://towardsdatascience.com/gaussian-mixture-model-clusterization-how-to-select-the-number-of-components-clusters-553bef45f6e4 (also determining appropriate number of clusters)<br/>\nhttps://github.com/vlavorini/ClusterCardinality/blob/master/Cluster%20Cardinality.ipynb<br/>\nhttps://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms<br/>\nhttps://stats.stackexchange.com/questions/349258/correct-number-of-components-in-gmm-according-to-bic-and-aic-plots<br/>\nhttps://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm<br/>\nhttps://grabngoinfo.com/how-to-decide-the-number-of-clusters-data-science-interview-questions-and-answers/\nBayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) scores","metadata":{}},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\nGMM = GaussianMixture(n_components=1, covariance_type='spherical', random_state=42)\nGMM.fit(x2_train_pca)\nscores = GMM.score_samples(x2_train_pca)\nthreshold = np.percentile(scores, 50)\npred1 = np.array([1 if score < threshold else 0 for score in GMM.score_samples(x2_train_pca)])\npred2 = np.array([1 if score < threshold else 0 for score in GMM.score_samples(x2_test_pca)])\nprint_stats(y2_train_pca, pred1, y2_test_pca, pred2, \"Gaussian Mixture Model (word embeddings)\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:14:12.24884Z","iopub.execute_input":"2024-04-03T06:14:12.250081Z","iopub.status.idle":"2024-04-03T06:14:12.637052Z","shell.execute_reply.started":"2024-04-03T06:14:12.25002Z","shell.execute_reply":"2024-04-03T06:14:12.635291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Means\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py <br />\nhttps://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/ <br />\nhttps://stackoverflow.com/a/69024239 <br/>\nhttps://stackoverflow.com/a/27586132 <br />\nhttps://www.dataknowsall.com/textclustering.html <br />\nhttps://www.kaggle.com/code/naren3256/kmeans-clustering-and-cluster-visualization-in-3d/notebook <br />\nhttps://medium.com/@jwbtmf/visualizing-data-using-k-means-clustering-unsupervised-machine-learning-8b59eabfcd3d <br />\nFor choosing the appropriate number of clusters, we can use Elbow method or Silhouette score. Let's use Elbow method.","metadata":{}},{"cell_type":"code","source":"# finding number of clusters with Elbow method\n# very long\n# from sklearn.cluster import KMeans\n\n# ks = range(1, 10)\n# inertias = []\n# for k in ks:\n#     kmeans = KMeans(n_clusters=k)\n#     # Fit model to samples\n#     kmeans.fit(x2_train)\n#     # Append the inertia to the list of inertias\n#     inertias.append(kmeans.inertia_)\n\n# # Plot ks vs inertias\n# plt.plot(ks, inertias, '-o')\n# plt.xlabel('number of clusters')\n# plt.ylabel('inertia')\n# plt.xticks(ks)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T05:51:49.430523Z","iopub.execute_input":"2024-04-03T05:51:49.431214Z","iopub.status.idle":"2024-04-03T05:51:49.436291Z","shell.execute_reply.started":"2024-04-03T05:51:49.431169Z","shell.execute_reply":"2024-04-03T05:51:49.435332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\nkmeans = KMeans(n_clusters=2)\nlabels = kmeans.fit_predict(x2_train_pca)\n\n# anomalies are far from the clusters' centers\ncentroids = kmeans.cluster_centers_\ndistances = cdist(x2_train_pca, centroids, 'euclidean')\nmin_distances = distances[np.arange(len(distances)), labels]\nthreshold = np.percentile(min_distances, 50)\npred1 = np.array([1 if d > threshold else 0 for d in min_distances])\n\nlabels = kmeans.predict(x2_test_pca)\ncentroids = kmeans.cluster_centers_\ndistances = cdist(x2_test_pca, centroids, 'euclidean')\nmin_distances = distances[np.arange(len(distances)), labels]\npred2 = np.array([1 if d > threshold else 0 for d in min_distances])\n\nprint_stats(y2_train_pca, pred1, y2_test_pca, pred2, \"KMeans\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:20:41.033845Z","iopub.execute_input":"2024-04-03T06:20:41.03433Z","iopub.status.idle":"2024-04-03T06:20:42.317609Z","shell.execute_reply.started":"2024-04-03T06:20:41.034286Z","shell.execute_reply":"2024-04-03T06:20:42.31634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep learning (unsupervised)\n(subset of machine learning algorithms)<br/>\n[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\ndef print_stats(predictions, X, y, title):\n    # Generate the classification report\n    report = classification_report(y, predictions, target_names=['Ham', 'Spam'])\n    print(report)\n\n    # Generate the confusion matrix\n    conf = confusion_matrix(y, predictions)\n    plt.title(title)\n    ax= plt.subplot()\n    sns.heatmap(conf, annot=True, fmt=\"\", linewidths=2, cmap=\"Greens\")\n    ax.set_xlabel('Predicted');\n    ax.set_ylabel('Real');\n    ax.xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax.yaxis.set_ticklabels(['Ham', 'Spam']);\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:54:55.272705Z","iopub.execute_input":"2024-04-03T06:54:55.273264Z","iopub.status.idle":"2024-04-03T06:54:55.28302Z","shell.execute_reply.started":"2024-04-03T06:54:55.27322Z","shell.execute_reply":"2024-04-03T06:54:55.281392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Scaling\nscaler = StandardScaler()\nx3_train = scaler.fit_transform(x3_train)\nx3_test = scaler.transform(x3_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:56:52.118308Z","iopub.execute_input":"2024-04-03T06:56:52.119749Z","iopub.status.idle":"2024-04-03T06:56:52.162732Z","shell.execute_reply.started":"2024-04-03T06:56:52.119686Z","shell.execute_reply":"2024-04-03T06:56:52.161536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Autoencoder\nhttps://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, losses\n\nX_train,X_valid,Y_train,Y_valid = train_test_split(x3_train, y3_train, random_state=42, test_size=0.2)\n\n# Keep only the normal data for the training dataset\nX_train_normal = X_train[Y_train == 0]\nX_valid_normal = X_valid[Y_valid == 0]\n\n# Input layer\ninput = tf.keras.layers.Input(shape=(x3_train.shape[1],))\n\n# Encoder layers\nencoder = Sequential([\n    layers.Dense(256, activation='relu'),  \n    layers.Dense(128, activation='relu'),  \n    layers.Dense(64, activation='relu'),  \n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(4, activation='relu')])(input)\n\n# Decoder layers\ndecoder = tf.keras.Sequential([\n    layers.Dense(8, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(300, activation=\"sigmoid\")])(encoder)\n\n# Create the autoencoder\nautoencoder = tf.keras.Model(inputs=input, outputs=decoder)\nautoencoder.compile(optimizer='adam', loss='mae')\nhistory = autoencoder.fit(X_train_normal, X_train_normal, \n                          epochs=20, \n                          batch_size=256,\n                          validation_data=(X_valid_normal, X_valid_normal),\n                          shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:56:53.958847Z","iopub.execute_input":"2024-04-03T06:56:53.959431Z","iopub.status.idle":"2024-04-03T06:57:00.475072Z","shell.execute_reply.started":"2024-04-03T06:56:53.959373Z","shell.execute_reply":"2024-04-03T06:57:00.473745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate MAE loss for validation data\nvalid_predictions = autoencoder.predict(X_valid_normal)\nvalid_loss = np.mean(np.abs(valid_predictions - X_valid_normal), axis=1)\nthreshold = np.percentile(valid_loss, 50)\n\n# Detect anomalies in the train set\ntrain_predictions = autoencoder.predict(x3_train)\ntrain_loss = np.mean(np.abs(train_predictions - x3_train), axis=1)\ntrain_loss = train_loss.reshape((-1))\npredictions = np.array([1 if loss > threshold else 0 for loss in train_loss])\nprint_stats(predictions, x3_train, y3_train, \"Autoencoder on train set\")\n\n# Detect anomalies in the test set\ntest_predictions = autoencoder.predict(x3_test)\ntest_loss = np.mean(np.abs(test_predictions - x3_test), axis=1)\ntest_loss = test_loss.reshape((-1))\npredictions = np.array([1 if loss > threshold else 0 for loss in test_loss])\nprint_stats(predictions, x3_test, y3_test, \"Autoencoder on test set\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T06:57:03.637475Z","iopub.execute_input":"2024-04-03T06:57:03.637967Z","iopub.status.idle":"2024-04-03T06:57:06.174428Z","shell.execute_reply.started":"2024-04-03T06:57:03.637924Z","shell.execute_reply":"2024-04-03T06:57:06.17313Z"},"trusted":true},"execution_count":null,"outputs":[]}]}