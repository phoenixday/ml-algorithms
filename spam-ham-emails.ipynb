{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kapusharinka/spam-ham-emails?scriptVersionId=144418713\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport time\nfrom pprint import pprint\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-27T08:27:06.568856Z","iopub.execute_input":"2023-09-27T08:27:06.570027Z","iopub.status.idle":"2023-09-27T08:27:06.577832Z","shell.execute_reply.started":"2023-09-27T08:27:06.569977Z","shell.execute_reply":"2023-09-27T08:27:06.576208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Articles about ML\nhttps://github.com/christianversloot/machine-learning-articles/tree/main","metadata":{}},{"cell_type":"markdown","source":"# Preparing datasets\nSo here are three datasets with emails. Here I'm showing their contents and concatenate them in one dataset.","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('/kaggle/input/email-spam-dataset/lingSpam.csv')\ndata1.info()\ndata1.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:06.743738Z","iopub.execute_input":"2023-09-27T08:27:06.744572Z","iopub.status.idle":"2023-09-27T08:27:07.017501Z","shell.execute_reply.started":"2023-09-27T08:27:06.744526Z","shell.execute_reply":"2023-09-27T08:27:07.01561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2 = pd.read_csv('/kaggle/input/email-spam-dataset/enronSpamSubset.csv')\ndata2.info()\ndata2.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.020104Z","iopub.execute_input":"2023-09-27T08:27:07.020472Z","iopub.status.idle":"2023-09-27T08:27:07.203485Z","shell.execute_reply.started":"2023-09-27T08:27:07.020437Z","shell.execute_reply":"2023-09-27T08:27:07.201956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = pd.read_csv('/kaggle/input/email-spam-dataset/completeSpamAssassin.csv')\ndata3.info()\ndata3.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.206301Z","iopub.execute_input":"2023-09-27T08:27:07.207632Z","iopub.status.idle":"2023-09-27T08:27:07.37292Z","shell.execute_reply.started":"2023-09-27T08:27:07.207568Z","shell.execute_reply":"2023-09-27T08:27:07.371338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete unneeded columns\ndata1.drop(\"Unnamed: 0\",inplace=True,axis=1)\ndata2.drop([\"Unnamed: 0\",\"Unnamed: 0.1\"],inplace=True,axis=1)\ndata3.drop(\"Unnamed: 0\",inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.375061Z","iopub.execute_input":"2023-09-27T08:27:07.37556Z","iopub.status.idle":"2023-09-27T08:27:07.38837Z","shell.execute_reply.started":"2023-09-27T08:27:07.375515Z","shell.execute_reply":"2023-09-27T08:27:07.386797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove rows where the text in 'text_column' is longer than max_length\ndef filter_by_body_length(df):\n    max_length = 100000\n    return df[df['Body'].apply(lambda x: len(str(x)) <= max_length)]\n\ndata1 = filter_by_body_length(data1)\ndata2 = filter_by_body_length(data2)\ndata3 = filter_by_body_length(data3)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.39153Z","iopub.execute_input":"2023-09-27T08:27:07.393183Z","iopub.status.idle":"2023-09-27T08:27:07.421546Z","shell.execute_reply.started":"2023-09-27T08:27:07.39307Z","shell.execute_reply":"2023-09-27T08:27:07.420135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate data and take a small fraction\n# data = pd.concat([data1,data2,data3],axis=0).sample(frac=0.05)\ndata = data1.sample(frac=0.3)\n# remove missing values (NaN)\ndata.dropna(inplace=True)\ndata.info()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.423305Z","iopub.execute_input":"2023-09-27T08:27:07.423757Z","iopub.status.idle":"2023-09-27T08:27:07.456944Z","shell.execute_reply.started":"2023-09-27T08:27:07.423716Z","shell.execute_reply":"2023-09-27T08:27:07.455166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nax = sns.countplot(x=data[\"Label\"], palette=(\"Pastel2\"))\nabs_values = data[\"Label\"].value_counts(ascending=False).values\nax.bar_label(container=ax.containers[0], labels=abs_values)\nax.set_xticklabels(['hams', 'spams'])\n\nplt.xlabel(None)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.458984Z","iopub.execute_input":"2023-09-27T08:27:07.459408Z","iopub.status.idle":"2023-09-27T08:27:07.679276Z","shell.execute_reply.started":"2023-09-27T08:27:07.459369Z","shell.execute_reply":"2023-09-27T08:27:07.677623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emails = data[\"Body\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.681142Z","iopub.execute_input":"2023-09-27T08:27:07.681538Z","iopub.status.idle":"2023-09-27T08:27:07.68902Z","shell.execute_reply.started":"2023-09-27T08:27:07.6815Z","shell.execute_reply":"2023-09-27T08:27:07.68705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text preprocessing\nHere I'm removing unneeded characters, like HTML tags, emails etc.","metadata":{}},{"cell_type":"code","source":"import re\n\n# remove emails\nemails = [re.sub('\\S*@\\S*\\s?', '', text) for text in emails]\n# remove url links\nemails = [re.sub('\\S*(http[s]?://|www\\.)\\S*', '', text) for text in emails]\n# remove HTML tags\nemails = [re.sub(r\"'<.*?>'\", \"\", text) for text in emails]\n# remove special characters and numbers\nemails = [re.sub(\"[^a-zA-Z]\",\" \",text) for text in emails]\n# remove too short (2- characters) words\nemails = [re.sub(r\"\\b\\w{1,2}\\b\", \"\",text) for text in emails]\n# and too long (17+ characters) \nemails = [re.sub(r\"\\b\\w{17,}\\b\", \"\",text) for text in emails]\n# remove multiple spaces\nemails = [re.sub(' +', ' ', text).strip() for text in emails]\n# lower\nemails = [text.lower() for text in emails]\n\nemails[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:07.690855Z","iopub.execute_input":"2023-09-27T08:27:07.691254Z","iopub.status.idle":"2023-09-27T08:27:09.295548Z","shell.execute_reply.started":"2023-09-27T08:27:07.691217Z","shell.execute_reply":"2023-09-27T08:27:09.293917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization and lemmatization\nTokenization: [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) (page 180)\n<ul>\n    <li>one-hot encoding of tokens</li>\n    <li>token embedding</li>\n</ul>\nFor more, look at <i>Deep Learning</i> section below.","metadata":{}},{"cell_type":"markdown","source":"We can choose between stemming or lemmatization - lemmatizators are slower, but change tenses and nouns. <br/>\nhttps://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n\nFirstly I used NLTK lemmatization, but it is very slow on my computer, so I tried SpaCy instead (https://spacy.io).","metadata":{}},{"cell_type":"code","source":"custom_stopwords = ['subject', 'empty', 'email', 'mail', 'enron', 'linux', 'list', 'get', 'http', 'vince', 'com', 'org', 'www', 'etc', 'ect', 'edu', 'hou', 'would', 'need']","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:09.297148Z","iopub.execute_input":"2023-09-27T08:27:09.297508Z","iopub.status.idle":"2023-09-27T08:27:09.30513Z","shell.execute_reply.started":"2023-09-27T08:27:09.297472Z","shell.execute_reply":"2023-09-27T08:27:09.303507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using NLTK\nEverything is very slow, but still works.","metadata":{}},{"cell_type":"code","source":"# tokenization\n\n# import nltk\n# emails = [nltk.word_tokenize(text) for text in emails]\n# emails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:09.310844Z","iopub.execute_input":"2023-09-27T08:27:09.311492Z","iopub.status.idle":"2023-09-27T08:27:09.321585Z","shell.execute_reply.started":"2023-09-27T08:27:09.31145Z","shell.execute_reply":"2023-09-27T08:27:09.320174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove stopwords\n\n# stopwords = nltk.corpus.stopwords.words(\"english\")\n# stopwords.extend(custom_stopwords)\n# emails = [[word for word in text if word not in stopwords] for text in emails]\n# emails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:09.323244Z","iopub.execute_input":"2023-09-27T08:27:09.323658Z","iopub.status.idle":"2023-09-27T08:27:09.335345Z","shell.execute_reply.started":"2023-09-27T08:27:09.323619Z","shell.execute_reply":"2023-09-27T08:27:09.333926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatization\n# very long\n\n# from nltk.stem import WordNetLemmatizer\n# from nltk.corpus import wordnet\n\n# def get_wordnet_pos(word):\n#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n#     tag_dict = {\"J\": wordnet.ADJ,\n#                 \"N\": wordnet.NOUN,\n#                 \"V\": wordnet.VERB,\n#                 \"R\": wordnet.ADV}\n#     tag = nltk.pos_tag([word])[0][1][0].upper()\n#     return tag_dict.get(tag, wordnet.NOUN)\n\n# nltk.data.path.append('/kaggle/input/corpora/')\n# lemmatizer = WordNetLemmatizer()\n# emails = [[lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text] for text in emails]\n# emails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:09.337441Z","iopub.execute_input":"2023-09-27T08:27:09.337835Z","iopub.status.idle":"2023-09-27T08:27:09.350295Z","shell.execute_reply.started":"2023-09-27T08:27:09.337798Z","shell.execute_reply":"2023-09-27T08:27:09.34864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using SpaCy\nhttps://stackoverflow.com/a/75215495","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# remove stop words\nfor word in custom_stopwords:\n    nlp.vocab[word].is_stop = True","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:09.352407Z","iopub.execute_input":"2023-09-27T08:27:09.352811Z","iopub.status.idle":"2023-09-27T08:27:10.351103Z","shell.execute_reply.started":"2023-09-27T08:27:09.352773Z","shell.execute_reply":"2023-09-27T08:27:10.350003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatization\nemails = [[token.lemma_ for token in nlp(text) if not token.is_stop] for text in emails]\nemails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:27:10.352687Z","iopub.execute_input":"2023-09-27T08:27:10.353995Z","iopub.status.idle":"2023-09-27T08:28:08.171881Z","shell.execute_reply.started":"2023-09-27T08:27:10.353947Z","shell.execute_reply":"2023-09-27T08:28:08.170125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction \nCreating a vector of features (words) for each email. <br />\nOpenAI:\n> Both CountVectorizer and TF-IDF (Term Frequency-Inverse Document Frequency) from scikit-learn are popular techniques for feature extraction in text data like emails, and each has its own merits.\n> \n> CountVectorizer creates a Bag of Words (BoW) model, where the features are the counts of each word in the document. This method is simple and easy to implement but can give more importance to words that appear frequently, regardless of their significance in distinguishing spam from non-spam emails.\n> \n> TF-IDF, on the other hand, takes into account not only the frequency of a word in a document but also its inverse frequency across all documents. This means that words that are common across all emails will receive lower weights, while words that are unique to specific emails will receive higher weights. This can be advantageous for spam detection, as spam emails often contain specific words or phrases that are less common in legitimate emails.\n> \n> In general, TF-IDF tends to work better than CountVectorizer for spam detection because it can better capture the importance of different words. However, the choice between the two methods will depend on the specific characteristics of the dataset and the problem you're trying to solve. It's a good idea to experiment with both techniques and evaluate their performance on your dataset using cross-validation or a separate validation set. This will help you determine which method works best for your particular spam detection task.","metadata":{}},{"cell_type":"code","source":"# bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# ngram_range=(1,2) means that the model will consider bigrams too\n# min_df=0.003 means that the model will not consider rare words\ncount_vectorizer = CountVectorizer(max_features=25000, ngram_range=(1,1), min_df=0.003, max_df=0.9)\nbag_of_words = count_vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(bag_of_words.shape)\ncount_vectorizer.get_feature_names_out()[:10] # first 10 in alphabetical order","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:08.174163Z","iopub.execute_input":"2023-09-27T08:28:08.174596Z","iopub.status.idle":"2023-09-27T08:28:08.555989Z","shell.execute_reply.started":"2023-09-27T08:28:08.174557Z","shell.execute_reply":"2023-09-27T08:28:08.55446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_vectorizer = TfidfVectorizer(max_features=25000, ngram_range=(1,1), min_df=0.003, max_df=0.9)\ntf_idf = tf_vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(tf_idf.shape)\ntf_vectorizer.get_feature_names_out()[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:08.560408Z","iopub.execute_input":"2023-09-27T08:28:08.5609Z","iopub.status.idle":"2023-09-27T08:28:08.967514Z","shell.execute_reply.started":"2023-09-27T08:28:08.560859Z","shell.execute_reply":"2023-09-27T08:28:08.965743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word embeddings\nA word embedding is a vector, that tries to capture \"meaning\" of a word/sentence/document etc... Some of the algorithms are provided by Word2Vec, GloVe. I used one provided by SpaCy.","metadata":{}},{"cell_type":"markdown","source":"## Word2vec\nhttp://jalammar.github.io/illustrated-word2vec/<br/>\nhttps://www.freecodecamp.org/news/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3/<br/>\nhttps://www.advancinganalytics.co.uk/blog/2022/7/22/using-machine-learning-to-perform-text-clustering<br/>\nhttps://aclanthology.org/I17-2006.pdf#:~:text=More%20specically%2C%20we%20show%20that%20the%20number%20of,in%20degradation%20of%20quality%20of%20learned%20word%20embeddings.\n### Using Gensim\nNot working yet.<br/>","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nword2vec_model = Word2Vec(emails, vector_size=1000, min_count=1, workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:08.969592Z","iopub.execute_input":"2023-09-27T08:28:08.970767Z","iopub.status.idle":"2023-09-27T08:28:14.228535Z","shell.execute_reply.started":"2023-09-27T08:28:08.970721Z","shell.execute_reply":"2023-09-27T08:28:14.227001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_email_vector(i):\n    return np.mean(np.array(word2vec_model.wv[emails[i]]), axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:14.230425Z","iopub.execute_input":"2023-09-27T08:28:14.23095Z","iopub.status.idle":"2023-09-27T08:28:14.238356Z","shell.execute_reply.started":"2023-09-27T08:28:14.230892Z","shell.execute_reply":"2023-09-27T08:28:14.236905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial\n\nfirst_ham = np.where(data['Label'] == 0)[0][0]\nsecond_ham = np.where(data['Label'] == 0)[0][1]\nfirst_spam = np.where(data['Label'] == 1)[0][0]\n\nsim = 1 - spatial.distance.cosine(get_email_vector(first_ham), get_email_vector(second_ham))\nprint(sim)\n\nsim = 1 - spatial.distance.cosine(get_email_vector(first_ham), get_email_vector(first_spam))\nprint(sim)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:14.240319Z","iopub.execute_input":"2023-09-27T08:28:14.241461Z","iopub.status.idle":"2023-09-27T08:28:14.26071Z","shell.execute_reply.started":"2023-09-27T08:28:14.241416Z","shell.execute_reply":"2023-09-27T08:28:14.259268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using SpaCy\nAlso check [sense2vec](http://https://github.com/explosion/sense2vec)","metadata":{}},{"cell_type":"code","source":"nlp = spacy.blank(\"en\")\nnlp.from_disk('/kaggle/input/en-core-web-lg/en_core_web_lg/en_core_web_lg-3.6.0')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:14.262589Z","iopub.execute_input":"2023-09-27T08:28:14.262978Z","iopub.status.idle":"2023-09-27T08:28:17.079366Z","shell.execute_reply.started":"2023-09-27T08:28:14.26294Z","shell.execute_reply":"2023-09-27T08:28:17.078307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just some playing.<br/>\nhttps://www.kaggle.com/code/hassanamin/learning-spacy-basics","metadata":{}},{"cell_type":"code","source":"# Document similarity\ndoc1 = nlp(\"I like guinea pigs\")\ndoc2 = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\nprint(doc1.similarity(doc2))\n\ndoc1 = nlp(\"I like guinea pigs\")\ndoc2 = nlp(\"I like cats\")\nprint(doc1.similarity(doc2))\n\n# Compare two tokens\ndoc = nlp(\"I like vegetables and buckwheat.\")\ntoken1 = doc[2]\ntoken2 = doc[4]\nprint(\"Token 1, Token 2: \",token1,\", \",token2)\nprint(token1.similarity(token2))\n\n# Compare a document with a token\ndoc = nlp(\"king\")\ntoken = nlp(\"prince\")[0]\nprint(doc.similarity(token))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:17.08112Z","iopub.execute_input":"2023-09-27T08:28:17.081647Z","iopub.status.idle":"2023-09-27T08:28:17.099453Z","shell.execute_reply.started":"2023-09-27T08:28:17.081594Z","shell.execute_reply":"2023-09-27T08:28:17.097483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating vectors for each email. We take a vector for each word from email, and finding the average vector for a whole email.","metadata":{}},{"cell_type":"code","source":"email_embeddings = []\nfor email in emails:\n    spacy_doc = nlp(' '.join(email))\n    avg_vector = sum([token.vector for token in spacy_doc]) / len(email)\n    email_embeddings.append(avg_vector)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:17.103711Z","iopub.execute_input":"2023-09-27T08:28:17.10414Z","iopub.status.idle":"2023-09-27T08:28:20.187882Z","shell.execute_reply.started":"2023-09-27T08:28:17.104102Z","shell.execute_reply":"2023-09-27T08:28:20.186534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More playing.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\nham1 = np.where(data['Label'] == 0)[0][0]\nham2 = np.where(data['Label'] == 0)[0][1]\nspam = np.where(data['Label'] == 1)[0][0]\n\nprint(\"\\nHam 1:\", data.iloc[ham1]['Body'][:500])\nprint(\"\\nHam 2:\", data.iloc[ham2]['Body'][:500])\nprint(\"\\nSpam:\", data.iloc[spam]['Body'][:500])\n\nham1_doc = nlp(' '.join(emails[ham1]))\nham2_doc = nlp(' '.join(emails[ham2]))\nspam_doc = nlp(' '.join(emails[spam]))\nprint(\"Similarity between ham 1 and ham 2:\", ham1_doc.similarity(ham2_doc))\nprint(\"Similarity between ham 1 and spam:\", ham1_doc.similarity(spam_doc))\nprint(\"Similarity between ham 2 and spam:\", ham2_doc.similarity(spam_doc))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:20.190314Z","iopub.execute_input":"2023-09-27T08:28:20.190786Z","iopub.status.idle":"2023-09-27T08:28:20.214449Z","shell.execute_reply.started":"2023-09-27T08:28:20.190742Z","shell.execute_reply":"2023-09-27T08:28:20.213014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word cloud\nHere are word clouds for spams and hams with the most frequent words, created with TF-IDF vectorizer.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\ndata['Tokens'] = emails\n\n# spams\nspams = data.loc[data['Label'] == 1, ['Tokens']]\nspam_x = tf_vectorizer.fit_transform([\" \".join(text) for text in spams['Tokens']]).toarray()\n\ndf = pd.DataFrame(spam_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\ndf.head(10)\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\nplt.title('Spams', fontsize = 40)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:20.216557Z","iopub.execute_input":"2023-09-27T08:28:20.216991Z","iopub.status.idle":"2023-09-27T08:28:21.208025Z","shell.execute_reply.started":"2023-09-27T08:28:20.216954Z","shell.execute_reply":"2023-09-27T08:28:21.205629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hams\nhams = data.loc[data['Label'] == 0, ['Tokens']]\nham_x = tf_vectorizer.fit_transform([\" \".join(text) for text in hams['Tokens']]).toarray()\n\ndf = pd.DataFrame(ham_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\nplt.title('Hams', fontsize = 40)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:21.211063Z","iopub.execute_input":"2023-09-27T08:28:21.212497Z","iopub.status.idle":"2023-09-27T08:28:25.471171Z","shell.execute_reply.started":"2023-09-27T08:28:21.212415Z","shell.execute_reply":"2023-09-27T08:28:25.469129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split to train and test data\nThe split is needed for supervised algorithms and supervised deep learning. I'm going to expriment with supervised algorithms, trying each on CounVectorizer and on TF-IDF.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split to train and test data for CountVectorizer\nx_train,x_test,y_train,y_test = train_test_split(bag_of_words, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)\n\n# split to train and test data for TF-IDF\nx2_train,x2_test,y2_train,y2_test = train_test_split(tf_idf, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)\n\n# split to train and test data for  embeddings\nx3_train,x3_test,y3_train,y3_test = train_test_split(email_embeddings, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:25.473535Z","iopub.execute_input":"2023-09-27T08:28:25.474639Z","iopub.status.idle":"2023-09-27T08:28:25.534974Z","shell.execute_reply.started":"2023-09-27T08:28:25.474587Z","shell.execute_reply":"2023-09-27T08:28:25.53345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification algorithms (supervised)\nhttps://towardsdatascience.com/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2 <br />\nAs we can see, the worse results are given by Naive Bayes with CountVectorizer. Other algorithms are dealing more or less well. The best results are given by VotingClassifier. <br />\nWe see that embeddings indeed performs better.\n## Evaluation metrics:\n**Accuracy** = (True Positives + True Negatives) / (True Positives + False Positives + True Negatives + False Negatives) <br />\nAccuracy measures the proportion of correct predictions made by the model out of the total number of predictions.\n\n\n**Precision** = True Positives / (True Positives + False Positives) <br />\nPrecision measures the proportion of true positive predictions out of all the positive predictions made by the model.\n\n\n**Recall** = True Positives / (True Positives + False Negatives) <br />\nRecall measures the proportion of true positive predictions to the number of actual positives (true positives + false negatives).\n\n\n**F1 Score** = 2 * (Precision * Recall) / (Precision + Recall) <br />\nAn F1 score reaches its best value at 1 (perfect precision and recall) and its worst value at 0.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n\ndef print_stats(algorithm, title, x_train_data, x_test_data, y_train_data, y_test_data): \n    \n    # actually perform classification\n    y_pred = algorithm.predict(x_test_data) \n\n    # Thus in binary classification, the count of \n    # true negatives is 0,0 \n    # false negatives is 1,0\n    # true positives is 1,1\n    # false positives is 0,1\n    conf = confusion_matrix(y_pred=y_pred,y_true=y_test_data)\n\n    plt.title(title)\n    ax= plt.subplot()\n    sns.heatmap(conf, annot=True, fmt=\"\", linewidths=2, cmap=\"Greens\")\n    ax.set_xlabel('Predicted');\n    ax.set_ylabel('Real');\n    ax.xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax.yaxis.set_ticklabels(['Ham', 'Spam']);\n    plt.show()\n    \n    tn, fp, fn, tp = conf.ravel()\n    print(\"Accuracy on training data: {:.2f}%\".format(100 * algorithm.score(x_train_data,y_train_data)))\n    print(\"Accuracy on testing data: {:.2f}%\".format(100 * algorithm.score(x_test_data,y_test_data)))\n    print(\"Precision: {:.2f}%\".format(100 * precision_score(y_pred, y_test_data)))\n    print(\"Recall: {:.2f}%\".format(100 * recall_score(y_pred, y_test_data)))\n    print(\"F1 Score: {:.2f}%\".format(100 * f1_score(y_pred, y_test_data)))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:25.536955Z","iopub.execute_input":"2023-09-27T08:28:25.537355Z","iopub.status.idle":"2023-09-27T08:28:25.552759Z","shell.execute_reply.started":"2023-09-27T08:28:25.537318Z","shell.execute_reply":"2023-09-27T08:28:25.550527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nNB = GaussianNB()\nNB.fit(x_train,y_train)\nprint_stats(NB,\"Gaussian Naive Bayes (bag of words)\",x_train,x_test,y_train,y_test)\n\nNB2 = GaussianNB()\nNB2.fit(x2_train,y2_train)\nprint_stats(NB2,\"Gaussian Naive Bayes (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\nNB3 = GaussianNB()\nNB3.fit(x3_train,y3_train)\nprint_stats(NB3,\"Gaussian Naive Bayes (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:25.570251Z","iopub.execute_input":"2023-09-27T08:28:25.570746Z","iopub.status.idle":"2023-09-27T08:28:26.792827Z","shell.execute_reply.started":"2023-09-27T08:28:25.570706Z","shell.execute_reply":"2023-09-27T08:28:26.79125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\nMNB = MultinomialNB()\nMNB.fit(x_train,y_train)\nprint_stats(MNB,\"Multinomial Naive Bayes (bag of words)\",x_train,x_test,y_train,y_test)\n\nMNB2 = MultinomialNB()\nMNB2.fit(x2_train,y2_train)\nprint_stats(MNB2,\"Multinomial Naive Bayes (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\n# MNB3 = MultinomialNB()\n# MNB3.fit(x3_train,y3_train)\n# print_stats(MNB3,\"Multinomial Naive Bayes (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:26.794496Z","iopub.execute_input":"2023-09-27T08:28:26.794896Z","iopub.status.idle":"2023-09-27T08:28:27.553797Z","shell.execute_reply.started":"2023-09-27T08:28:26.794858Z","shell.execute_reply":"2023-09-27T08:28:27.551994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(max_iter=1000)\nLR.fit(x_train, y_train)\nprint_stats(LR,\"Logistic Regression (bag of words)\",x_train,x_test,y_train,y_test)\n\nLR2 = LogisticRegression(max_iter=1000)\nLR2.fit(x2_train,y2_train)\nprint_stats(LR2,\"Logistic Regression (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\nLR3 = LogisticRegression(max_iter=1000)\nLR3.fit(x3_train,y3_train)\nprint_stats(LR3,\"Logistic Regression (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:27.556714Z","iopub.execute_input":"2023-09-27T08:28:27.55796Z","iopub.status.idle":"2023-09-27T08:28:29.056725Z","shell.execute_reply.started":"2023-09-27T08:28:27.557878Z","shell.execute_reply":"2023-09-27T08:28:29.054921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\nKNN.fit(x_train, y_train)\nprint_stats(KNN,\"K Neighbors Classifier (bag of words)\",x_train,x_test,y_train,y_test)\n\nKNN2 = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\nKNN2.fit(x2_train, y2_train)\nprint_stats(KNN2,\"K Neighbors Classifier (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\nKNN3 = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\nKNN3.fit(x3_train, y3_train)\nprint_stats(KNN3,\"K Neighbors Classifier (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:29.059658Z","iopub.execute_input":"2023-09-27T08:28:29.060923Z","iopub.status.idle":"2023-09-27T08:28:31.566097Z","shell.execute_reply.started":"2023-09-27T08:28:29.060848Z","shell.execute_reply":"2023-09-27T08:28:31.564984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\n\nSVC = LinearSVC(C=0.001)\nSVC.fit(x_train, y_train)\nprint_stats(SVC,\"Linear Support Vector Classification (bag of words)\",x_train,x_test,y_train,y_test)\n\nSVC2 = LinearSVC(C=30)\nSVC2.fit(x2_train,y2_train)\nprint_stats(SVC2,\"Linear Support Vector Classification (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\nSVC3 = LinearSVC(C=10)\nSVC3.fit(x3_train,y3_train)\nprint_stats(SVC3,\"Linear Support Vector Classification (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:31.568981Z","iopub.execute_input":"2023-09-27T08:28:31.569537Z","iopub.status.idle":"2023-09-27T08:28:32.911597Z","shell.execute_reply.started":"2023-09-27T08:28:31.56948Z","shell.execute_reply":"2023-09-27T08:28:32.909725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nCLF = DecisionTreeClassifier()\nCLF.fit(x_train, y_train)\nprint_stats(CLF,\"Decision Tree Classifier (bag of words)\",x_train,x_test,y_train,y_test)\n\nCLF2 = DecisionTreeClassifier()\nCLF2.fit(x2_train, y2_train)\nprint_stats(CLF2,\"Decision Tree Classifier (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\nCLF3 = DecisionTreeClassifier()\nCLF3.fit(x3_train, y3_train)\nprint_stats(CLF3,\"Decision Tree Classifier (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:32.91454Z","iopub.execute_input":"2023-09-27T08:28:32.915692Z","iopub.status.idle":"2023-09-27T08:28:35.080673Z","shell.execute_reply.started":"2023-09-27T08:28:32.915614Z","shell.execute_reply":"2023-09-27T08:28:35.078905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# n_estimators = number of decision trees\nRF = RandomForestClassifier(n_estimators=100, max_depth=50)\nRF.fit(x_train, y_train)\nprint_stats(RF,\"Random Forest Classifier (bag of words)\",x_train,x_test,y_train,y_test)\n\nRF2 = RandomForestClassifier(n_estimators=100, max_depth=50)\nRF2.fit(x2_train,y2_train)\nprint_stats(RF2,\"Random Forest Classifier (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\nRF3 = RandomForestClassifier(n_estimators=100, max_depth=50)\nRF3.fit(x3_train,y3_train)\nprint_stats(RF3,\"Random Forest Classifier (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:35.084729Z","iopub.execute_input":"2023-09-27T08:28:35.085285Z","iopub.status.idle":"2023-09-27T08:28:38.634854Z","shell.execute_reply.started":"2023-09-27T08:28:35.085242Z","shell.execute_reply":"2023-09-27T08:28:38.633195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\n\nEVC = VotingClassifier(estimators=[('LR',LR),('RF',RF),('SVC',SVC),('KNN', KNN),('CLF', CLF)], voting='hard')\nEVC.fit(x_train, y_train)\nprint_stats(EVC,\"Voting Classifier (bag of words)\",x_train,x_test,y_train,y_test)\n\nEVC2 = VotingClassifier(estimators=[('LR',LR2),('RF',RF2),('SVC',SVC2),('KNN', KNN2),('CLF', CLF2)], voting='hard')\nEVC2.fit(x2_train,y2_train)\nprint_stats(EVC2,\"Voting Classifier (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)\n\nEVC3 = VotingClassifier(estimators=[('LR',LR3),('RF',RF3),('SVC',SVC3),('KNN2', KNN3),('CLF2', CLF3)], voting='hard')\nEVC3.fit(x3_train,y3_train)\nprint_stats(EVC3,\"Voting Classifier (embeddings)\",x3_train,x3_test,y3_train,y3_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:38.637478Z","iopub.execute_input":"2023-09-27T08:28:38.637898Z","iopub.status.idle":"2023-09-27T08:28:46.067266Z","shell.execute_reply.started":"2023-09-27T08:28:38.637858Z","shell.execute_reply":"2023-09-27T08:28:46.065405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsupervised algorithms \nI'm not sure if it's possible to classify emails on spams and hams using unsupervised algorithms. <br />\nBut we can use LDA (or NMF) for extracting the topics, or K-Means for finding clusters, which can be helpful. <br />\n## Topic modelling (LDA and NMF)\nhttps://www.dataknowsall.com/topicmodels.html <br />\nhttps://github.com/ashishsalunkhe/Topic-Modeling-using-LDA-and-K-Means-Clustering <br />\nhttps://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2 <br />\nhttps://towardsdatascience.com/the-ultimate-guide-to-clustering-algorithms-and-topic-modeling-3a65129df324 <br />\nhttps://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5 <br />\nWe can choose between Sklearn LDA and Gensim LDA. I chosed the Sklearn's for now, since it's faster and gives better results. https://medium.com/@benzgreer/sklearn-lda-vs-gensim-lda-691a9f2e9ab7 <br />\nThere's an alternative for LDA, NMF. So I tried them both. <br />\nhttps://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df","metadata":{}},{"cell_type":"code","source":"def explore_topics(algorithm, chosen_vectorizer):\n    results_dict = {}\n    top_n_words = 15\n    feature_names = chosen_vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(algorithm.components_):\n        top_n_words = 15\n        top_words = [feature_names[i] for i in topic.argsort()[-top_n_words:]]\n        results_dict[f'Topic {topic_idx}'] = top_words\n    return pd.DataFrame.from_dict(results_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:46.068789Z","iopub.execute_input":"2023-09-27T08:28:46.069197Z","iopub.status.idle":"2023-09-27T08:28:46.0788Z","shell.execute_reply.started":"2023-09-27T08:28:46.069159Z","shell.execute_reply":"2023-09-27T08:28:46.076941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\n\ndef lda_features(model, n_components):\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n    features = lda.fit_transform(model)\n    return features, lda","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:46.08093Z","iopub.execute_input":"2023-09-27T08:28:46.082715Z","iopub.status.idle":"2023-09-27T08:28:46.099715Z","shell.execute_reply.started":"2023-09-27T08:28:46.082636Z","shell.execute_reply":"2023-09-27T08:28:46.097705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's better first to choose the best number of topics with eiter perplexity, or coherence. <br />\n* **Perplexity**: Lower the perplexity better the model.\n* **Coherence**: Higher the topic coherence, the topic is more human interpretable. ","metadata":{}},{"cell_type":"code","source":"def elbow_method_lda(model, max_components, title):\n    ks = range(1, max_components)\n    perplexities = []\n    for k in ks:\n        lda = LatentDirichletAllocation(n_components=k, random_state=42)\n        # Fit model to samples\n        lda.fit(model)\n        # Append the perplexity\n        perplexities.append(lda.perplexity(model))\n\n    # Plot \n    plt.plot(ks, perplexities, '-o')\n    plt.title(title)\n    plt.xlabel('number of topics')\n    plt.ylabel('perplexity')\n    plt.xticks(ks)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:46.102248Z","iopub.execute_input":"2023-09-27T08:28:46.103316Z","iopub.status.idle":"2023-09-27T08:28:46.119442Z","shell.execute_reply.started":"2023-09-27T08:28:46.103251Z","shell.execute_reply":"2023-09-27T08:28:46.118182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# very long\nelbow_method_lda(bag_of_words, 15, \"Topics for Count vectorizer\")\nelbow_method_lda(tf_idf, 15, \"Topics for TD-IDF vectorizer\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:28:46.121293Z","iopub.execute_input":"2023-09-27T08:28:46.121746Z","iopub.status.idle":"2023-09-27T08:31:08.736952Z","shell.execute_reply.started":"2023-09-27T08:28:46.1217Z","shell.execute_reply":"2023-09-27T08:31:08.735495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_features_x, lda_x = lda_features(bag_of_words, 3)\nexplore_topics(lda_x, count_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:08.738878Z","iopub.execute_input":"2023-09-27T08:31:08.739293Z","iopub.status.idle":"2023-09-27T08:31:15.741532Z","shell.execute_reply.started":"2023-09-27T08:31:08.739255Z","shell.execute_reply":"2023-09-27T08:31:15.740142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_features_x2, lda_x2 = lda_features(tf_idf, 3)\nexplore_topics(lda_x2, tf_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:15.743587Z","iopub.execute_input":"2023-09-27T08:31:15.744054Z","iopub.status.idle":"2023-09-27T08:31:19.106229Z","shell.execute_reply.started":"2023-09-27T08:31:15.744011Z","shell.execute_reply":"2023-09-27T08:31:19.104842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import NMF\n\nnmf  = NMF(n_components = 3)\nnmf.fit_transform(bag_of_words)\nexplore_topics(nmf, count_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.108274Z","iopub.execute_input":"2023-09-27T08:31:19.108654Z","iopub.status.idle":"2023-09-27T08:31:19.754699Z","shell.execute_reply.started":"2023-09-27T08:31:19.108617Z","shell.execute_reply":"2023-09-27T08:31:19.752969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gensim LDA and coherence","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.756984Z","iopub.execute_input":"2023-09-27T08:31:19.758451Z","iopub.status.idle":"2023-09-27T08:31:19.7653Z","shell.execute_reply.started":"2023-09-27T08:31:19.758374Z","shell.execute_reply":"2023-09-27T08:31:19.763496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The process\n![process](https://miro.medium.com/v2/resize:fit:720/format:webp/1*2W8wMBiGEdHBCevqZ0d79g.png)\n<br/>(taken from https://towardsdatascience.com/a-friendly-introduction-to-text-clustering-fa996bcefd04)","metadata":{}},{"cell_type":"markdown","source":"## Clustering metrics\n* **Davies-Bouldin Index**: a lower DBI value indicates better clustering performance, as it signifies less scatter within clusters and more separation between them.\n* **Silhouette Score**: a value that ranges from -1 to 1; a higher silhouette score indicates that the data point is well-matched to its own cluster and poorly matched to other clusters.\n* **Calinski-Harabasz**: to be added","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import davies_bouldin_score, silhouette_score\n\ndef print_clustering_metrics(model, labels):\n    db_index = davies_bouldin_score(model, labels)\n    sil_score = silhouette_score(model,labels)\n    print(\"Davies-Bouldin Index: {:.5f}\".format(db_index))\n    print(\"Silhouette Score: {:.5f}\".format(sil_score))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.768258Z","iopub.execute_input":"2023-09-27T08:31:19.769453Z","iopub.status.idle":"2023-09-27T08:31:19.782142Z","shell.execute_reply.started":"2023-09-27T08:31:19.769379Z","shell.execute_reply":"2023-09-27T08:31:19.780306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_plot_with_clusters(labels):\n    # Count the number of elements in each cluster\n    unique_labels, counts = np.unique(labels, return_counts=True)\n\n    # Create a DataFrame from the unique labels and counts\n    data = pd.DataFrame({'Cluster ID': unique_labels, 'Number of Elements': counts})\n\n    # Plotting the bar chart\n    ax = sns.barplot(x='Cluster ID', y='Number of Elements', data=data, palette=(\"Pastel1\"))\n    plt.xlabel('Cluster ID')\n    plt.ylabel('Number of Elements')\n    plt.title('Number of Elements in Each Cluster')\n\n    # Add numbers on top of bars\n    for i, count in enumerate(counts):\n        ax.text(i, count+0.5, str(count), ha='center')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.7849Z","iopub.execute_input":"2023-09-27T08:31:19.786568Z","iopub.status.idle":"2023-09-27T08:31:19.807639Z","shell.execute_reply.started":"2023-09-27T08:31:19.78649Z","shell.execute_reply":"2023-09-27T08:31:19.805874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_emails_from_clusters(labels):\n    numpy_labels = np.array(labels)\n    for label in np.unique(labels):\n        print(\"==================================\")\n        print(\"Several emails from the cluster\", label)\n        print(\"==================================\")\n        for i in range(0, 5):\n            length = len(np.where(numpy_labels == label)[0])\n            if i < length:\n                pos = np.where(numpy_labels == label)[0][i]\n                print(data.iloc[pos]['Body'][:1000])\n                print('----------------------------------')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.810624Z","iopub.execute_input":"2023-09-27T08:31:19.812223Z","iopub.status.idle":"2023-09-27T08:31:19.831263Z","shell.execute_reply.started":"2023-09-27T08:31:19.812151Z","shell.execute_reply":"2023-09-27T08:31:19.830195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## StandardScaler and PCA\n\nUsing **StandardScaler** ensures that all features have a mean of 0 and a standard deviation of 1. <br />\n**PCA** can help reduce noise. <br />\nStandardScaler and PCA can be useful when working with KMeans and DBSCAN because these algorithms are sensitive to the scale of the input features and the dimensionality of the data. <br />\nBut I'm not sure, if they're needed.","metadata":{}},{"cell_type":"code","source":"# will leave them here for now\n\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n\n# scaler = StandardScaler()\n# x2_scaled = scaler.fit_transform(x2)\n\n# # initialize PCA with 2 components\n# pca = PCA(n_components=2, random_state=42)\n# pca_vecs = pca.fit_transform(x2_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.833067Z","iopub.execute_input":"2023-09-27T08:31:19.833844Z","iopub.status.idle":"2023-09-27T08:31:19.844985Z","shell.execute_reply.started":"2023-09-27T08:31:19.833788Z","shell.execute_reply":"2023-09-27T08:31:19.843414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [t-SNE](https://lvdmaaten.github.io/tsne/)\nReducing to 2 dimensions for visualizing,","metadata":{}},{"cell_type":"markdown","source":"## K-Means (or MiniBatchKMeans)\nWe can choose between K-Means and MiniBatchKMeans, which is faster. But the results of K-Means seem more meaningful. <br />\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py <br />\nhttps://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/ <br />\nhttps://stackoverflow.com/a/69024239 <br/>\nhttps://stackoverflow.com/a/27586132 <br />\nhttps://www.dataknowsall.com/textclustering.html <br />\nhttps://www.kaggle.com/code/naren3256/kmeans-clustering-and-cluster-visualization-in-3d/notebook <br />\nhttps://medium.com/@jwbtmf/visualizing-data-using-k-means-clustering-unsupervised-machine-learning-8b59eabfcd3d <br />\nFor choosing the appropriate number of clusters, we can use Elbow method or Silhouette score. Let's use Elbow method.","metadata":{}},{"cell_type":"code","source":"# finding number of clusters with Elbow method\n# very long\nfrom sklearn.cluster import KMeans\n\ndef elbow_method(model, max_clusters, title):\n    ks = range(1, max_clusters)\n    inertias = []\n    for k in ks:\n        kmeans = KMeans(n_clusters=k)\n        # Fit model to samples\n        kmeans.fit(model)\n        # Append the inertia to the list of inertias\n        inertias.append(kmeans.inertia_)\n\n    # Plot ks vs inertias\n    plt.plot(ks, inertias, '-o')\n    plt.title(title)\n    plt.xlabel('number of clusters')\n    plt.ylabel('inertia')\n    plt.xticks(ks)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.847075Z","iopub.execute_input":"2023-09-27T08:31:19.847538Z","iopub.status.idle":"2023-09-27T08:31:19.859953Z","shell.execute_reply.started":"2023-09-27T08:31:19.8475Z","shell.execute_reply":"2023-09-27T08:31:19.85856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# actually KMeans\n# using clusters, where there's some elbow.\ndef kmeans_with_clusters(n_clusters, model, chosen_vectorizer):\n    kmeans = KMeans(n_clusters=n_clusters)\n    labels = kmeans.fit_predict(model)\n\n    # top 15 words from each cluster\n    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n    terms = chosen_vectorizer.get_feature_names_out()\n    results_dict = {}\n    for i in range(n_clusters):\n        terms_list = []\n        for ind in order_centroids[i, :15]:  \n            terms_list.append(terms[ind])\n        results_dict[f'Cluster {i}'] = terms_list\n    df_clusters = pd.DataFrame.from_dict(results_dict)\n    return df_clusters","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.861472Z","iopub.execute_input":"2023-09-27T08:31:19.862732Z","iopub.status.idle":"2023-09-27T08:31:19.879485Z","shell.execute_reply.started":"2023-09-27T08:31:19.862684Z","shell.execute_reply":"2023-09-27T08:31:19.878408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elbow_method(bag_of_words, 30, \"With Count vectorizer\")\nelbow_method(tf_idf, 30, \"With TD-IDF vectorizer\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:31:19.880906Z","iopub.execute_input":"2023-09-27T08:31:19.882209Z","iopub.status.idle":"2023-09-27T08:35:03.62158Z","shell.execute_reply.started":"2023-09-27T08:31:19.882152Z","shell.execute_reply":"2023-09-27T08:35:03.619887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# actually kmeans\n# for count vectorizer\nkmeans_with_clusters(8, bag_of_words, count_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:03.623846Z","iopub.execute_input":"2023-09-27T08:35:03.625169Z","iopub.status.idle":"2023-09-27T08:35:06.025108Z","shell.execute_reply.started":"2023-09-27T08:35:03.625109Z","shell.execute_reply":"2023-09-27T08:35:06.023318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for td-idf vectorizer\nkmeans_with_clusters(7, tf_idf, tf_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:06.02693Z","iopub.execute_input":"2023-09-27T08:35:06.027532Z","iopub.status.idle":"2023-09-27T08:35:09.041074Z","shell.execute_reply.started":"2023-09-27T08:35:06.027488Z","shell.execute_reply":"2023-09-27T08:35:09.039509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the same, but using lda_features\nelbow_method(lda_features_x, 15, \"With Count vectorizer\")\nelbow_method(lda_features_x2, 15, \"With TD-IDF vectorizer\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:09.043049Z","iopub.execute_input":"2023-09-27T08:35:09.043452Z","iopub.status.idle":"2023-09-27T08:35:10.817057Z","shell.execute_reply.started":"2023-09-27T08:35:09.043416Z","shell.execute_reply":"2023-09-27T08:35:10.815323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans_with_clusters(3, lda_features_x, count_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:10.819047Z","iopub.execute_input":"2023-09-27T08:35:10.81942Z","iopub.status.idle":"2023-09-27T08:35:11.415418Z","shell.execute_reply.started":"2023-09-27T08:35:10.819384Z","shell.execute_reply":"2023-09-27T08:35:11.413992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans_with_clusters(3, lda_features_x2, tf_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:11.417279Z","iopub.execute_input":"2023-09-27T08:35:11.417812Z","iopub.status.idle":"2023-09-27T08:35:11.471005Z","shell.execute_reply.started":"2023-09-27T08:35:11.417756Z","shell.execute_reply":"2023-09-27T08:35:11.469957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using email embeddings from SpaCy \nelbow_method(email_embeddings, 30, \"With SpaCy embeddings\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:11.472297Z","iopub.execute_input":"2023-09-27T08:35:11.47349Z","iopub.status.idle":"2023-09-27T08:35:33.405481Z","shell.execute_reply.started":"2023-09-27T08:35:11.473438Z","shell.execute_reply":"2023-09-27T08:35:33.404172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=6)\nlabels = kmeans.fit_predict(email_embeddings)\nprint_clustering_metrics(email_embeddings, labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:33.407231Z","iopub.execute_input":"2023-09-27T08:35:33.408395Z","iopub.status.idle":"2023-09-27T08:35:33.881972Z","shell.execute_reply.started":"2023-09-27T08:35:33.408341Z","shell.execute_reply":"2023-09-27T08:35:33.880624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DBSCAN\n\nhttps://youtu.be/RDZUdRSDOok <br/>\nhttps://dev.to/wmisingo/machine-learning-text-clustering-with-dbscan-399n <br/>\nhttps://towardsdatascience.com/how-dbscan-works-and-why-should-i-use-it-443b4a191c80 <br/>\nhttps://www.advancinganalytics.co.uk/blog/2022/7/22/using-machine-learning-to-perform-text-clustering <br/>\nhttps://www.kaggle.com/code/karthik3890/text-clustering\n\n### Determining best eps\nhttp://www.sefidian.com/2022/12/18/how-to-determine-epsilon-and-minpts-parameters-of-dbscan-clustering/","metadata":{}},{"cell_type":"code","source":"# choosing eps\nfrom sklearn.neighbors import NearestNeighbors\n\nmin_samples = 70\n\n# Compute the distance to the min_samples-th nearest neighbor for each point\nnearest_neighbors = NearestNeighbors(n_neighbors=min_samples)\nnearest_neighbors.fit(email_embeddings)\ndistances, _ = nearest_neighbors.kneighbors(email_embeddings)\n\n# Sort the distances\ndistances = np.sort(distances, axis=0)[:, min_samples-1]\n\n# Plot the sorted distances\nplt.plot(distances)\nplt.xlabel('Sample index')\nplt.ylabel('Distance to min_samples-th nearest neighbor')\nplt.title('Elbow Method For Optimal eps')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:33.890283Z","iopub.execute_input":"2023-09-27T08:35:33.894893Z","iopub.status.idle":"2023-09-27T08:35:34.253785Z","shell.execute_reply.started":"2023-09-27T08:35:33.894803Z","shell.execute_reply":"2023-09-27T08:35:34.252469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=12, min_samples=min_samples)\ndbscan.fit(email_embeddings)\n\nclusters = np.unique(dbscan.labels_)\nnoise = np.sum(np.array(dbscan.labels_) == -1, axis=0)\nprint('Number of clusters: %d' % len(clusters))\nprint('Number of noise points: %d' % noise)\nprint_clustering_metrics(email_embeddings, dbscan.labels_)\nshow_plot_with_clusters(dbscan.labels_)\nshow_emails_from_clusters(dbscan.labels_)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:34.255566Z","iopub.execute_input":"2023-09-27T08:35:34.25959Z","iopub.status.idle":"2023-09-27T08:35:34.590043Z","shell.execute_reply.started":"2023-09-27T08:35:34.259519Z","shell.execute_reply":"2023-09-27T08:35:34.587064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Agglomerative (hierarchical) clustering\nhttps://www.javatpoint.com/hierarchical-clustering-in-machine-learning","metadata":{}},{"cell_type":"code","source":"# finding the optimal number of clusters using the dendrogram  \n# long\nimport scipy.cluster.hierarchy as shc  \n\ndendro = shc.dendrogram(shc.linkage(email_embeddings, method=\"ward\"))  \nplt.title(\"Dendrogram Plot\")  \nplt.ylabel(\"Euclidean Distances\")  \nplt.xlabel(\"Emails\")  \nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:34.592184Z","iopub.execute_input":"2023-09-27T08:35:34.592569Z","iopub.status.idle":"2023-09-27T08:35:51.429329Z","shell.execute_reply.started":"2023-09-27T08:35:34.592533Z","shell.execute_reply":"2023-09-27T08:35:51.427997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters = 3)\nac_labels = ac.fit_predict(email_embeddings)\nprint_clustering_metrics(email_embeddings, ac_labels)\nshow_plot_with_clusters(ac_labels)\nshow_emails_from_clusters(ac_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:51.431292Z","iopub.execute_input":"2023-09-27T08:35:51.431733Z","iopub.status.idle":"2023-09-27T08:35:51.820329Z","shell.execute_reply.started":"2023-09-27T08:35:51.431693Z","shell.execute_reply":"2023-09-27T08:35:51.818947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spectral clustering","metadata":{}},{"cell_type":"code","source":"# from sklearn.cluster import SpectralClustering\n\n# sc = SpectralClustering(n_clusters=3, assign_labels='discretize', random_state=0)\n# sc_labels = sc.fit_predict(bag_of_words)\n# print_clustering_metrics(bag_of_words, sc_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:51.822065Z","iopub.execute_input":"2023-09-27T08:35:51.823611Z","iopub.status.idle":"2023-09-27T08:35:51.830724Z","shell.execute_reply.started":"2023-09-27T08:35:51.823556Z","shell.execute_reply":"2023-09-27T08:35:51.829111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian Mixture Model\nhttps://vitalflux.com/gaussian-mixture-models-what-are-they-when-to-use/ <br/>\nhttps://towardsdatascience.com/gaussian-mixture-model-clusterization-how-to-select-the-number-of-components-clusters-553bef45f6e4 (also determining appropriate number of clusters)<br/>\nhttps://github.com/vlavorini/ClusterCardinality/blob/master/Cluster%20Cardinality.ipynb<br/>\nhttps://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms<br/>\nhttps://stats.stackexchange.com/questions/349258/correct-number-of-components-in-gmm-according-to-bic-and-aic-plots<br/>\nhttps://stats.stackexchange.com/questions/368560/elbow-test-using-aic-bic-for-identifying-number-of-clusters-using-gmm<br/>\nhttps://grabngoinfo.com/how-to-decide-the-number-of-clusters-data-science-interview-questions-and-answers/\nBayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) scores","metadata":{}},{"cell_type":"code","source":"# from sklearn.mixture import GaussianMixture\n\n# n_components = range(1, 4)\n# bics = []\n# for n in n_components:\n#     gmm = GaussianMixture(n_components=n, random_state=42)\n#     gmm.fit(tf_idf)\n#     bics.append(gmm.bic(tf_idf))\n# plt.plot(n_components, bics)\n# plt.xlabel(\"Number of Clusters\")\n# plt.ylabel(\"BIC Score\")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:51.832483Z","iopub.execute_input":"2023-09-27T08:35:51.8333Z","iopub.status.idle":"2023-09-27T08:35:51.842008Z","shell.execute_reply.started":"2023-09-27T08:35:51.833247Z","shell.execute_reply":"2023-09-27T08:35:51.840405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm_labels = gmm.fit_predict(email_embeddings)\nprint_clustering_metrics(email_embeddings, gmm_labels)\nshow_plot_with_clusters(gmm_labels)\nshow_emails_from_clusters(gmm_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:51.843818Z","iopub.execute_input":"2023-09-27T08:35:51.844247Z","iopub.status.idle":"2023-09-27T08:35:52.395805Z","shell.execute_reply.started":"2023-09-27T08:35:51.84421Z","shell.execute_reply":"2023-09-27T08:35:52.393895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Anomaly detection\nhttps://blog.paperspace.com/anomaly-detection-isolation-forest/<br/>\nEach entry is either 1 for normal data or -1 for an anomaly.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\niff = IsolationForest(contamination=0.1)\npredictions = iff.fit_predict(email_embeddings)\nprint(len(predictions[predictions == -1]), \" outliers from \", len(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:52.397852Z","iopub.execute_input":"2023-09-27T08:35:52.398406Z","iopub.status.idle":"2023-09-27T08:35:52.91501Z","shell.execute_reply.started":"2023-09-27T08:35:52.39835Z","shell.execute_reply":"2023-09-27T08:35:52.913324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = iff.decision_function(email_embeddings)\nscores_sorted = np.argsort(scores)\nscores[scores_sorted][:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:52.917322Z","iopub.execute_input":"2023-09-27T08:35:52.917902Z","iopub.status.idle":"2023-09-27T08:35:53.054535Z","shell.execute_reply.started":"2023-09-27T08:35:52.917839Z","shell.execute_reply":"2023-09-27T08:35:53.053147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the \"most\" outlier\ndata.iloc[scores_sorted[0]]['Body'][:1500]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.056234Z","iopub.execute_input":"2023-09-27T08:35:53.056597Z","iopub.status.idle":"2023-09-27T08:35:53.066668Z","shell.execute_reply.started":"2023-09-27T08:35:53.056561Z","shell.execute_reply":"2023-09-27T08:35:53.065197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the most \"ordinary\"\ndata.iloc[scores_sorted[-1]]['Body'][:1500]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.068936Z","iopub.execute_input":"2023-09-27T08:35:53.070498Z","iopub.status.idle":"2023-09-27T08:35:53.082174Z","shell.execute_reply.started":"2023-09-27T08:35:53.070448Z","shell.execute_reply":"2023-09-27T08:35:53.080568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\n\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\npredictions = lof.fit_predict(email_embeddings)\nprint(len(predictions[predictions == -1]), \" outliers from \", len(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.084311Z","iopub.execute_input":"2023-09-27T08:35:53.084866Z","iopub.status.idle":"2023-09-27T08:35:53.140885Z","shell.execute_reply.started":"2023-09-27T08:35:53.084814Z","shell.execute_reply":"2023-09-27T08:35:53.138815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = lof.negative_outlier_factor_\nscores_sorted = np.argsort(scores)\nscores[scores_sorted][:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.143034Z","iopub.execute_input":"2023-09-27T08:35:53.14397Z","iopub.status.idle":"2023-09-27T08:35:53.157586Z","shell.execute_reply.started":"2023-09-27T08:35:53.143912Z","shell.execute_reply":"2023-09-27T08:35:53.155978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[scores_sorted[0]]['Body'][:1500]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.159925Z","iopub.execute_input":"2023-09-27T08:35:53.160849Z","iopub.status.idle":"2023-09-27T08:35:53.175138Z","shell.execute_reply.started":"2023-09-27T08:35:53.160794Z","shell.execute_reply":"2023-09-27T08:35:53.173291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[scores_sorted[-1]]['Body'][:1500]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.177685Z","iopub.execute_input":"2023-09-27T08:35:53.178612Z","iopub.status.idle":"2023-09-27T08:35:53.195998Z","shell.execute_reply.started":"2023-09-27T08:35:53.178558Z","shell.execute_reply":"2023-09-27T08:35:53.193978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import OneClassSVM\n\nocsvm = OneClassSVM(kernel='rbf', nu=0.1, gamma=0.1)\npredictions = ocsvm.fit_predict(email_embeddings)\nprint(len(predictions[predictions == -1]), \" outliers from \", len(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.199526Z","iopub.execute_input":"2023-09-27T08:35:53.200977Z","iopub.status.idle":"2023-09-27T08:35:53.4954Z","shell.execute_reply.started":"2023-09-27T08:35:53.200896Z","shell.execute_reply":"2023-09-27T08:35:53.494229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = ocsvm.decision_function(email_embeddings)\nscores_sorted = np.argsort(scores)\nscores[scores_sorted][:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.497357Z","iopub.execute_input":"2023-09-27T08:35:53.498144Z","iopub.status.idle":"2023-09-27T08:35:53.690379Z","shell.execute_reply.started":"2023-09-27T08:35:53.498101Z","shell.execute_reply":"2023-09-27T08:35:53.689067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[scores_sorted[0]]['Body'][:1500]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.692065Z","iopub.execute_input":"2023-09-27T08:35:53.692788Z","iopub.status.idle":"2023-09-27T08:35:53.703127Z","shell.execute_reply.started":"2023-09-27T08:35:53.692746Z","shell.execute_reply":"2023-09-27T08:35:53.701153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[scores_sorted[-1]]['Body'][:1500]","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.705336Z","iopub.execute_input":"2023-09-27T08:35:53.705776Z","iopub.status.idle":"2023-09-27T08:35:53.719588Z","shell.execute_reply.started":"2023-09-27T08:35:53.705736Z","shell.execute_reply":"2023-09-27T08:35:53.717912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep learning algorithms\n(subset of machine learning algorithms)<br/>\n### [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n<ul>\n    <li><b>shallow</b> vs <b>deep</b> methods, <b>feature engineering</b> (pages 17-18)</li>\n    <li><b>XGBoost</b> (gradient boosting machines) vs <b>Keras</b> (page 19)</li>\n    <li><b>History</b> object and overfitting (pages 74-76)</li>\n    <li>? <b>weight regularization</b> and <b>dropout</b> (pages 107-110)</li>\n    <li>? how do <b>2D convnets</b> and <b>MaxPooling2D</b> layer are working? (pages 122-129)</li>\n    <li>convnets and dealing with overfitting using <b>data augmentation</b> (page 159)</li>\n    <li><b>tokenization</b> (for more, look at <i>Tokenization</i> section above)</li>\n    <li><b>Embedding</b> layer (page 186)</li>\n    <li><b>SimpleRNN</b>, <b>LSTM</b> and <b>GRU</b></li>\n</ul>\n\n## Supervised Learning\n**CNNs** do not use output from previous layers directly to affect future layers (apart from the standard feed-forward propagation). An example would be an image, where each pixel can be independently processed. <br />\n**RNNs** maintain a kind of 'memory' of previous inputs. This is useful in sequential data where the position and context of an element matter, like in a sentence. <br />\nhttps://pruthivi.medium.com/spam-classification-using-deep-neural-network-architecture-129860a6b9fb<br />\nhttps://www.educba.com/tensorflow-sequential/","metadata":{}},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# define the CNN model\nembedding_dim = 100  # Size of the word embeddings\n\nmodel = Sequential([\n        # first parameter input_dim=3588 == input vocab\n        Embedding(3588, embedding_dim, input_length=3588),\n        Conv1D(128, 5, activation='relu'),\n        GlobalMaxPooling1D(),\n        Dense(64, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ], name=\"cnn_model\")\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:53.720825Z","iopub.execute_input":"2023-09-27T08:35:53.721244Z","iopub.status.idle":"2023-09-27T08:35:54.01629Z","shell.execute_reply.started":"2023-09-27T08:35:53.721187Z","shell.execute_reply":"2023-09-27T08:35:54.014691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 10\nbatch_size = 32\nhistory = model.fit(x2_train, y2_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.017848Z","iopub.execute_input":"2023-09-27T08:35:54.01826Z","iopub.status.idle":"2023-09-27T08:35:54.367057Z","shell.execute_reply.started":"2023-09-27T08:35:54.018214Z","shell.execute_reply":"2023-09-27T08:35:54.364062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\nprint(f\"Test set accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.369152Z","iopub.status.idle":"2023-09-27T08:35:54.369883Z","shell.execute_reply.started":"2023-09-27T08:35:54.369611Z","shell.execute_reply":"2023-09-27T08:35:54.369646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n# from tensorflow.keras.optimizers import Adam\n\n# model = Sequential()\n# model.add(Embedding(input_dim=3588, output_dim=100, input_length=3588))\n# model.add(LSTM(units=32))\n# model.add(Dense(1, activation='sigmoid'))\n\n# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.371647Z","iopub.status.idle":"2023-09-27T08:35:54.372141Z","shell.execute_reply.started":"2023-09-27T08:35:54.371885Z","shell.execute_reply":"2023-09-27T08:35:54.371909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RNN\n#### Theory\nhttps://machinelearningmastery.com/calculus-in-action-neural-networks/<br/>\nhttps://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them<br/>\nhttps://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/<br/>\nhttps://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n#### Tutorials\nhttps://victorzhou.com/blog/keras-rnn-tutorial/\n#### Documentation\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization<br/>\nhttps://keras.io/api/optimizers/","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import LSTM\nfrom keras.layers import Flatten\n\nmaxlen = 25\nmodel = Sequential([\n        Embedding(10000, 8, input_length=maxlen),\n        Flatten(),\n        Dense(1, activation='sigmoid')\n    ], name=\"rnn_model\")\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.374134Z","iopub.status.idle":"2023-09-27T08:35:54.374973Z","shell.execute_reply.started":"2023-09-27T08:35:54.374701Z","shell.execute_reply":"2023-09-27T08:35:54.374729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import preprocessing\n\n# x_train, x_test, y_train, y_test = train_test_split(emails, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)\n# x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) \n# x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n\n# train the model\nhistory = model.fit(x2_train, y2_train, epochs=10, batch_size=32, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.376305Z","iopub.status.idle":"2023-09-27T08:35:54.376723Z","shell.execute_reply.started":"2023-09-27T08:35:54.376514Z","shell.execute_reply":"2023-09-27T08:35:54.376536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\nprint(f\"Test set accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.378521Z","iopub.status.idle":"2023-09-27T08:35:54.379011Z","shell.execute_reply.started":"2023-09-27T08:35:54.378732Z","shell.execute_reply":"2023-09-27T08:35:54.378754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start_time = time.time()\n\n# # define the RNN model\n# embedding_dim = 100  # Size of the word embeddings\n\n# model = Sequential([\n#     Embedding(max_words, embedding_dim, input_length=max_length),\n#     SimpleRNN(128, activation='tanh', return_sequences=True),\n#     SimpleRNN(64, activation='tanh'),\n#     Dense(1, activation='sigmoid')\n# ])\n\n# model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n# model.summary()\n\n# # train the model\n# epochs = 10\n# batch_size = 32\n# history = model.fit(x2_train, y2_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n\n# # evaluate the model\n# loss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\n# print(f\"Test set accuracy: {accuracy:.4f}\")\n\n# end_time = time.time()\n# elapsed_time = end_time - start_time\n# print(\"Elapsed time: \", elapsed_time) ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.380688Z","iopub.status.idle":"2023-09-27T08:35:54.381209Z","shell.execute_reply.started":"2023-09-27T08:35:54.380898Z","shell.execute_reply":"2023-09-27T08:35:54.38092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LSTM\n# start_time = time.time()\n\n# # define the LSTM model\n# embedding_dim = 100  # Size of the word embeddings\n\n# model = Sequential([\n#     Embedding(max_words, embedding_dim, input_length=max_length),\n#     LSTM(128, return_sequences=True),\n#     LSTM(64),\n#     Dense(1, activation='sigmoid')\n# ])\n\n# model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n# model.summary()\n\n# # train the model\n# epochs = 10\n# batch_size = 32\n# history = model.fit(x2_train, y2_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n\n# # evaluate the model\n# loss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\n# print(f\"Test set accuracy: {accuracy:.4f}\")\n\n# end_time = time.time()\n# elapsed_time = end_time - start_time\n# print(\"Elapsed time: \", elapsed_time) ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.383815Z","iopub.status.idle":"2023-09-27T08:35:54.384342Z","shell.execute_reply.started":"2023-09-27T08:35:54.384048Z","shell.execute_reply":"2023-09-27T08:35:54.384104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unsupervised Learning\nOpenAI:\n> While unsupervised deep learning algorithms can help learn useful representations of the data, they typically need to be combined with a supervised classifier or clustering algorithm to perform the actual spam detection. For instance, you could use an autoencoder to learn a low-dimensional representation of the email data and then train a supervised classifier (e.g., logistic regression, SVM) on the extracted features to classify emails as spam or ham.","metadata":{}},{"cell_type":"markdown","source":"### Autoencoder","metadata":{}},{"cell_type":"code","source":"# from keras.layers import Input, Dense\n# from keras.models import Model\n\n# input_dim = x2.shape[1]\n# encoding_dim = 64  # The dimensionality of the latent space\n\n# # Define the encoder\n# input_data = Input(shape=(input_dim,))\n# encoded = Dense(encoding_dim, activation='relu')(input_data)\n\n# # Define the decoder\n# decoded = Dense(input_dim, activation='sigmoid')(encoded)\n\n# # Create the autoencoder model\n# autoencoder = Model(input_data, decoded)\n\n# # Create the encoder model\n# encoder = Model(input_data, encoded)\n\n# # Compile and train the autoencoder\n# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n# autoencoder.fit(x2, x2, epochs=50, batch_size=256, shuffle=True, \n#                 validation_data=(np.asarray(data[\"Label\"]), np.asarray(data[\"Label\"])))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:35:54.386576Z","iopub.status.idle":"2023-09-27T08:35:54.38711Z","shell.execute_reply.started":"2023-09-27T08:35:54.38684Z","shell.execute_reply":"2023-09-27T08:35:54.386866Z"},"trusted":true},"execution_count":null,"outputs":[]}]}