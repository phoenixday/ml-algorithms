{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kapusharinka/spam-ham-emails?scriptVersionId=136289104\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport time\nfrom pprint import pprint\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-03T14:00:38.017649Z","iopub.execute_input":"2023-07-03T14:00:38.01882Z","iopub.status.idle":"2023-07-03T14:00:38.025524Z","shell.execute_reply.started":"2023-07-03T14:00:38.018773Z","shell.execute_reply":"2023-07-03T14:00:38.024192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing datasets\nSo here are three datasets with emails. Here I'm showing their contents and concatenate them in one dataset.","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('/kaggle/input/email-spam-dataset/lingSpam.csv')\ndata1.info()\ndata1.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:38.134552Z","iopub.execute_input":"2023-07-03T14:00:38.134979Z","iopub.status.idle":"2023-07-03T14:00:38.625746Z","shell.execute_reply.started":"2023-07-03T14:00:38.134944Z","shell.execute_reply":"2023-07-03T14:00:38.62433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2 = pd.read_csv('/kaggle/input/email-spam-dataset/enronSpamSubset.csv')\ndata2.info()\ndata2.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:38.628123Z","iopub.execute_input":"2023-07-03T14:00:38.628788Z","iopub.status.idle":"2023-07-03T14:00:38.95072Z","shell.execute_reply.started":"2023-07-03T14:00:38.62874Z","shell.execute_reply":"2023-07-03T14:00:38.949317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = pd.read_csv('/kaggle/input/email-spam-dataset/completeSpamAssassin.csv')\ndata3.info()\ndata3.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:38.952444Z","iopub.execute_input":"2023-07-03T14:00:38.953252Z","iopub.status.idle":"2023-07-03T14:00:39.21168Z","shell.execute_reply.started":"2023-07-03T14:00:38.953182Z","shell.execute_reply":"2023-07-03T14:00:39.210447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete unneeded columns\ndata1.drop(\"Unnamed: 0\",inplace=True,axis=1)\ndata2.drop([\"Unnamed: 0\",\"Unnamed: 0.1\"],inplace=True,axis=1)\ndata3.drop(\"Unnamed: 0\",inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:39.215405Z","iopub.execute_input":"2023-07-03T14:00:39.216167Z","iopub.status.idle":"2023-07-03T14:00:39.227519Z","shell.execute_reply.started":"2023-07-03T14:00:39.216121Z","shell.execute_reply":"2023-07-03T14:00:39.225828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate data\n# data = pd.concat([data1,data2,data3],axis=0)\ndata = data1\n# remove missing values (NaN)\ndata.dropna(inplace=True)\ndata.info()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:39.229469Z","iopub.execute_input":"2023-07-03T14:00:39.229968Z","iopub.status.idle":"2023-07-03T14:00:39.257484Z","shell.execute_reply.started":"2023-07-03T14:00:39.229925Z","shell.execute_reply":"2023-07-03T14:00:39.256093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emails = data[\"Body\"]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:39.259569Z","iopub.execute_input":"2023-07-03T14:00:39.260017Z","iopub.status.idle":"2023-07-03T14:00:39.264771Z","shell.execute_reply.started":"2023-07-03T14:00:39.259971Z","shell.execute_reply":"2023-07-03T14:00:39.263747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text preprocessing\nHere I'm removing unneeded characters, like HTML tags, emails etc.","metadata":{}},{"cell_type":"code","source":"import re\n\n# remove emails\nemails = [re.sub('\\S*@\\S*\\s?', '', text) for text in emails]\n# remove url links\nemails = [re.sub('\\S*(http[s]?://|www\\.)\\S*', '', text) for text in emails]\n# remove HTML tags\nemails = [re.sub(r\"'<.*?>'\", \"\", text) for text in emails]\n# remove special characters and numbers\nemails = [re.sub(\"[^a-zA-Z]\",\" \",text) for text in emails]\n# remove too short (2- characters) words\nemails = [re.sub(r\"\\b\\w{1,2}\\b\", \"\",text) for text in emails]\n# and too long (17+ characters) \nemails = [re.sub(r\"\\b\\w{17,}\\b\", \"\",text) for text in emails]\n# lower\nemails = [text.lower() for text in emails]\n\nemails[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:39.266195Z","iopub.execute_input":"2023-07-03T14:00:39.266829Z","iopub.status.idle":"2023-07-03T14:00:47.416402Z","shell.execute_reply.started":"2023-07-03T14:00:39.26679Z","shell.execute_reply":"2023-07-03T14:00:47.414979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization and lemmatization\nTokenization: [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) (page 180)\n<ul>\n    <li>one-hot encoding of tokens</li>\n    <li>token embedding</li>\n</ul>\nFor more, look at <i>Deep Learning</i> section below.","metadata":{}},{"cell_type":"markdown","source":"We can choose between stemming or lemmatization - lemmatizators are slower, but change tenses and nouns. <br/>\nhttps://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n\nFirstly I used NLTK lemmatization, but it is very slow on my computer, so I tried SpaCy instead (https://spacy.io). But I need to investigate more, SpaCy isn't working faster either.","metadata":{}},{"cell_type":"markdown","source":"### Using NLTK","metadata":{}},{"cell_type":"code","source":"# tokenization\nimport nltk\nemails = [nltk.word_tokenize(text) for text in emails]\nemails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:00:47.418471Z","iopub.execute_input":"2023-07-03T14:00:47.419396Z","iopub.status.idle":"2023-07-03T14:01:03.061356Z","shell.execute_reply.started":"2023-07-03T14:00:47.419341Z","shell.execute_reply":"2023-07-03T14:01:03.060156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove stopwords\nstopwords = nltk.corpus.stopwords.words(\"english\")\nstopwords.extend(['subject', 'empty', 'email', 'mail', 'enron', 'linux', 'list', 'get', 'http', 'vince', 'com', 'org', 'www'])\nemails = [[word for word in text if word not in stopwords] for text in emails]\nemails[0][:15]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:01:03.062765Z","iopub.execute_input":"2023-07-03T14:01:03.063198Z","iopub.status.idle":"2023-07-03T14:01:10.600635Z","shell.execute_reply.started":"2023-07-03T14:01:03.063165Z","shell.execute_reply":"2023-07-03T14:01:10.59942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatization\n# very long, 15 minutes\nstart_time = time.time()\nnltk.data.path.append('/kaggle/input/corpora/')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    return tag_dict.get(tag, wordnet.NOUN)\n\nlemmatizer = WordNetLemmatizer()\nemails = [[lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text] for text in emails]\nemails[0][:15]\nend_time = time.time()\nprint(\"Time elapsed: \", end_time - start_time)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:01:27.075346Z","iopub.execute_input":"2023-07-03T14:01:27.075779Z","iopub.status.idle":"2023-07-03T14:07:49.219608Z","shell.execute_reply.started":"2023-07-03T14:01:27.075743Z","shell.execute_reply":"2023-07-03T14:07:49.218152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using SpaCy\nhttps://stackoverflow.com/a/75215495","metadata":{}},{"cell_type":"code","source":"# import spacy\n# from spacy.lang.en.stop_words import STOP_WORDS\n\n# nlp = spacy.load(\"en_core_web_sm\")\n# STOP_WORDS.update({'subject', 'empty', 'email', 'mail', 'enron', 'linux', 'list', 'get'})\n\n# lemmatized_emails = []\n\n# for email in emails:\n#     lemmatized_email = []\n#     for doc in nlp.pipe(email):\n#         lemmatized_email.extend([token.lemma_ for token in doc if not token.is_stop])\n#     lemmatized_emails.append(lemmatized_email)\n\n# print(lemmatized_emails[0])  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction \nCreating a vector of features (words) for each email. <br />\nOpenAI:\n> Both CountVectorizer and TF-IDF (Term Frequency-Inverse Document Frequency) from scikit-learn are popular techniques for feature extraction in text data like emails, and each has its own merits.\n> \n> CountVectorizer creates a Bag of Words (BoW) model, where the features are the counts of each word in the document. This method is simple and easy to implement but can give more importance to words that appear frequently, regardless of their significance in distinguishing spam from non-spam emails.\n> \n> TF-IDF, on the other hand, takes into account not only the frequency of a word in a document but also its inverse frequency across all documents. This means that words that are common across all emails will receive lower weights, while words that are unique to specific emails will receive higher weights. This can be advantageous for spam detection, as spam emails often contain specific words or phrases that are less common in legitimate emails.\n> \n> In general, TF-IDF tends to work better than CountVectorizer for spam detection because it can better capture the importance of different words. However, the choice between the two methods will depend on the specific characteristics of the dataset and the problem you're trying to solve. It's a good idea to experiment with both techniques and evaluate their performance on your dataset using cross-validation or a separate validation set. This will help you determine which method works best for your particular spam detection task.","metadata":{}},{"cell_type":"code","source":"# bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# ngram_range=(1,4) means that the model will consider bigrams, trigrams and quadgrams too\n# min_df=0.003 means that the model will not consider rare words\nvectorizer = CountVectorizer(max_features=25000, ngram_range=(1,4), min_df=0.003)\nx = vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(x.shape)\nvectorizer.get_feature_names_out()[:10] # first 10 in alphabetical order","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:07:56.188364Z","iopub.execute_input":"2023-07-03T14:07:56.188765Z","iopub.status.idle":"2023-07-03T14:08:11.922407Z","shell.execute_reply.started":"2023-07-03T14:07:56.18873Z","shell.execute_reply":"2023-07-03T14:08:11.918676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_vectorizer = TfidfVectorizer(max_features=25000, ngram_range=(1,4), min_df=0.003)\nx2 = tf_vectorizer.fit_transform([\" \".join(text) for text in emails]).toarray()\nprint(x2.shape)\ntf_vectorizer.get_feature_names_out()[:10]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:08:28.724564Z","iopub.execute_input":"2023-07-03T14:08:28.724989Z","iopub.status.idle":"2023-07-03T14:08:44.234632Z","shell.execute_reply.started":"2023-07-03T14:08:28.724951Z","shell.execute_reply":"2023-07-03T14:08:44.233574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2vec\nhttp://jalammar.github.io/illustrated-word2vec/<br/>\nhttps://www.freecodecamp.org/news/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3/","metadata":{}},{"cell_type":"markdown","source":"# Word cloud\nHere are word clouds for spams and hams with the most frequent words, created with TF-IDF vectorizer.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\ndata['Tokens'] = emails\n\n# spams\nspams = data.loc[data['Label'] == 1, ['Tokens']]\nspam_x = tf_vectorizer.fit_transform([\" \".join(text) for text in spams['Tokens']]).toarray()\n\ndf = pd.DataFrame(spam_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\ndf.head(10)\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\nplt.title('Spams', fontsize = 40)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:08:44.237286Z","iopub.execute_input":"2023-07-03T14:08:44.238193Z","iopub.status.idle":"2023-07-03T14:08:49.551917Z","shell.execute_reply.started":"2023-07-03T14:08:44.238138Z","shell.execute_reply":"2023-07-03T14:08:49.550192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hams\nhams = data.loc[data['Label'] == 0, ['Tokens']]\nham_x = tf_vectorizer.fit_transform([\" \".join(text) for text in hams['Tokens']]).toarray()\n\ndf = pd.DataFrame(ham_x.tolist(), columns=tf_vectorizer.get_feature_names_out())\n\nwordcloud = WordCloud(background_color='white', max_words=200,\n                      stopwords = STOPWORDS, collocations=True).generate_from_frequencies(df.T.sum(axis=1))\nplt.title('Hams', fontsize = 40)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:08:49.55337Z","iopub.execute_input":"2023-07-03T14:08:49.553754Z","iopub.status.idle":"2023-07-03T14:09:16.416322Z","shell.execute_reply.started":"2023-07-03T14:08:49.55372Z","shell.execute_reply":"2023-07-03T14:09:16.414746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split to train and test data\nThe split is needed for supervised algorithms. I'm going to expriment with supervised algorithms, trying each on CounVectorizer and on TF-IDF.","metadata":{}},{"cell_type":"code","source":"# split to train and test data for CountVectorizer\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)\n\n# split to train and test data for TF-IDF\nx2_train,x2_test,y2_train,y2_test = train_test_split(x2, np.asarray(data[\"Label\"]), random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:11:54.05498Z","iopub.execute_input":"2023-07-03T14:11:54.055728Z","iopub.status.idle":"2023-07-03T14:11:54.442129Z","shell.execute_reply.started":"2023-07-03T14:11:54.055665Z","shell.execute_reply":"2023-07-03T14:11:54.4406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification algorithms (supervised)\nhttps://towardsdatascience.com/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2 <br />\nAs we can see, the worse results are given by Naive Bayes with CountVectorizer. Other algorithms are dealing more or less well. The best results are given by VotingClassifier. <br />\nWe see that TF-IDF indeed performs better.\n## Evaluation metrics:\n**Accuracy** = (True Positives + True Negatives) / (True Positives + False Positives + True Negatives + False Negatives) <br />\nAccuracy measures the proportion of correct predictions made by the model out of the total number of predictions.\n\n\n**Precision** = True Positives / (True Positives + False Positives) <br />\nPrecision measures the proportion of true positive predictions out of all the positive predictions made by the model.\n\n\n**Recall** = True Positives / (True Positives + False Negatives) <br />\nIn the context of spam detection, recall indicates how well the classifier identifies spam emails out of all the actual spam emails.\n\n\n**F1 Score** = 2 * (Precision * Recall) / (Precision + Recall) <br />\nAn F1 score reaches its best value at 1 (perfect precision and recall) and its worst value at 0.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nimport seaborn\n\ndef print_stats(algorithm, title, x_train_data, x_test_data, y_train_data, y_test_data): \n    \n    # actually perform classification\n    y_pred = algorithm.predict(x_test_data) \n\n    # Thus in binary classification, the count of \n    # true negatives is 0,0 \n    # false negatives is 1,0\n    # true positives is 1,1\n    # false positives is 0,1\n    conf = confusion_matrix(y_pred=y_pred,y_true=y_test_data)\n\n    plt.title(title)\n    ax= plt.subplot()\n    seaborn.heatmap(conf, annot=True, fmt=\"\", linewidths=2, cmap=\"Greens\")\n    ax.set_xlabel('Predicted');\n    ax.set_ylabel('Real');\n    ax.xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax.yaxis.set_ticklabels(['Ham', 'Spam']);\n    plt.show()\n    \n    tn, fp, fn, tp = conf.ravel()\n    print(\"Accuracy on training data: {:.2f}%\".format(100 * algorithm.score(x_train_data,y_train_data)))\n    print(\"Accuracy on testing data: {:.2f}%\".format(100 * algorithm.score(x_test_data,y_test_data)))\n    print(\"Precision: {:.2f}%\".format(100 * precision_score(y_pred, y_test_data)))\n    print(\"Recall: {:.2f}%\".format(100 * recall_score(y_pred, y_test_data)))\n    print(\"F1 Score: {:.2f}%\".format(100 * f1_score(y_pred, y_test_data)))","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:12:06.136216Z","iopub.execute_input":"2023-07-03T14:12:06.136684Z","iopub.status.idle":"2023-07-03T14:12:06.466731Z","shell.execute_reply.started":"2023-07-03T14:12:06.13664Z","shell.execute_reply":"2023-07-03T14:12:06.465464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nNB = GaussianNB()\nNB.fit(x_train,y_train)\nprint_stats(NB,\"Gaussian Naive Bayes (bag of words)\",x_train,x_test,y_train,y_test)\n\nNB2 = GaussianNB()\nNB2.fit(x2_train,y2_train)\nprint_stats(NB2,\"Gaussian Naive Bayes (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:12:13.415245Z","iopub.execute_input":"2023-07-03T14:12:13.415671Z","iopub.status.idle":"2023-07-03T14:12:17.016916Z","shell.execute_reply.started":"2023-07-03T14:12:13.415632Z","shell.execute_reply":"2023-07-03T14:12:17.015557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\nMNB = MultinomialNB()\nMNB.fit(x_train,y_train)\nprint_stats(MNB,\"Multinomial Naive Bayes (bag of words)\",x_train,x_test,y_train,y_test)\n\nMNB2 = MultinomialNB()\nMNB2.fit(x2_train,y2_train)\nprint_stats(MNB2,\"Multinomial Naive Bayes (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:12:21.815905Z","iopub.execute_input":"2023-07-03T14:12:21.816459Z","iopub.status.idle":"2023-07-03T14:12:23.970568Z","shell.execute_reply.started":"2023-07-03T14:12:21.816409Z","shell.execute_reply":"2023-07-03T14:12:23.969065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(max_iter=1000)\nLR.fit(x_train, y_train)\nprint_stats(LR,\"Logistic Regression (bag of words)\",x_train,x_test,y_train,y_test)\n\nLR2 = LogisticRegression(max_iter=1000)\nLR2.fit(x2_train,y2_train)\nprint_stats(LR2,\"Logistic Regression (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:12:29.937586Z","iopub.execute_input":"2023-07-03T14:12:29.938009Z","iopub.status.idle":"2023-07-03T14:12:33.425396Z","shell.execute_reply.started":"2023-07-03T14:12:29.937965Z","shell.execute_reply":"2023-07-03T14:12:33.421294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# very long and not very accurate, 12 minutes\n# from sklearn.neighbors import KNeighborsClassifier\n# KNN = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n# KNN.fit(x_train, y_train)\n# print_stats(KNN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\n\nSVC = LinearSVC(C=0.001)\nSVC.fit(x_train, y_train)\nprint_stats(SVC,\"Linear Support Vector Classification (bag of words)\",x_train,x_test,y_train,y_test)\n\nSVC2 = LinearSVC(C=10)\nSVC2.fit(x2_train,y2_train)\nprint_stats(SVC2,\"Linear Support Vector Classification (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:12:37.721176Z","iopub.execute_input":"2023-07-03T14:12:37.72247Z","iopub.status.idle":"2023-07-03T14:12:39.339357Z","shell.execute_reply.started":"2023-07-03T14:12:37.722412Z","shell.execute_reply":"2023-07-03T14:12:39.337898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4 minutes\n# from sklearn.tree import DecisionTreeClassifier\n# CLF = DecisionTreeClassifier()\n# CLF.fit(x_train, y_train)\n# print_stats(CLF)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# n_estimators = number of decision trees\nRF = RandomForestClassifier(n_estimators=100, max_depth=50)\nRF.fit(x_train, y_train)\nprint_stats(RF,\"Random Forest Classifier (bag of words)\",x_train,x_test,y_train,y_test)\n\nRF2 = RandomForestClassifier(n_estimators=100, max_depth=50)\nRF2.fit(x2_train,y2_train)\nprint_stats(RF2,\"Random Forest Classifier (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:12:43.443985Z","iopub.execute_input":"2023-07-03T14:12:43.444495Z","iopub.status.idle":"2023-07-03T14:13:04.633057Z","shell.execute_reply.started":"2023-07-03T14:12:43.444439Z","shell.execute_reply":"2023-07-03T14:13:04.631698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\n\nEVC = VotingClassifier(estimators=[('MNB',MNB),('LR',LR),('RF',RF),('SVC',SVC)], voting='hard')\nEVC.fit(x_train, y_train)\nprint_stats(EVC,\"Voting Classifier (bag of words)\",x_train,x_test,y_train,y_test)\n\nEVC2 = VotingClassifier(estimators=[('MNB',MNB2),('LR',LR2),('RF',RF2),('SVC',SVC2)], voting='hard')\nEVC2.fit(x2_train,y2_train)\nprint_stats(EVC2,\"Voting Classifier (TF-IDF)\",x2_train,x2_test,y2_train,y2_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:13:21.411308Z","iopub.execute_input":"2023-07-03T14:13:21.411784Z","iopub.status.idle":"2023-07-03T14:13:48.473611Z","shell.execute_reply.started":"2023-07-03T14:13:21.411739Z","shell.execute_reply":"2023-07-03T14:13:48.472033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsupervised algorithms \nI'm not sure if it's possible to classify emails on spams and hams using unsupervised algorithms. <br />\nBut we can use LDA (or NMF) for extracting the topics, or K-Means for finding clusters, which can be helpful. <br />\n## Topic modelling\nhttps://www.dataknowsall.com/topicmodels.html <br />\nhttps://github.com/ashishsalunkhe/Topic-Modeling-using-LDA-and-K-Means-Clustering <br />\nhttps://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2 <br />\nhttps://towardsdatascience.com/the-ultimate-guide-to-clustering-algorithms-and-topic-modeling-3a65129df324 <br />\nhttps://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5 <br />\nWe can choose between Sklearn LDA and Gensim LDA. I chosed the Sklearn's for now, since it's faster and gives better results. https://medium.com/@benzgreer/sklearn-lda-vs-gensim-lda-691a9f2e9ab7 <br />\nThere's an alternative for LDA, NMF. So I tried them both. <br />\nhttps://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df","metadata":{}},{"cell_type":"code","source":"def explore_topics(algorithm):\n    results_dict = {}\n    top_n_words = 15\n    feature_names = tf_vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(algorithm.components_):\n        top_n_words = 15\n        top_words = [feature_names[i] for i in topic.argsort()[-top_n_words:]]\n        results_dict[f'Topic {topic_idx}'] = top_words\n    return pd.DataFrame.from_dict(results_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Actually, it's better first to choose the best number of topics with eiter perplexity, or coherence. But I will leave it for now.<br />\n* **Perplexity**: Lower the perplexity better the model.\n* **Coherence**: Higher the topic coherence, the topic is more human interpretable. ","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_components=10, random_state=42)\nlda.fit_transform(x2)\nexplore_topics(lda)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import NMF\n\nnmf  = NMF(n_components = 10)\nnmf.fit_transform(x2)\nexplore_topics(nmf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## StandardScaler and PCA\n\nUsing **StandardScaler** ensures that all features have a mean of 0 and a standard deviation of 1. <br />\n**PCA** can help reduce noise. <br />\nStandardScaler and PCA can be useful when working with KMeans and DBSCAN because these algorithms are sensitive to the scale of the input features and the dimensionality of the data. <br />\nBut I'm not sure, if they're needed.","metadata":{}},{"cell_type":"code","source":"# will leave them here for now\n\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n\n# scaler = StandardScaler()\n# x2_scaled = scaler.fit_transform(x2)\n\n# # initialize PCA with 2 components\n# pca = PCA(n_components=2, random_state=42)\n# pca_vecs = pca.fit_transform(x2_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Means (or MiniBatchKMeans)\nWe can choose between K-Means and MiniBatchKMeans, which is faster. But the results of K-Means seem more meaningful. <br />\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py <br />\nhttps://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/ <br />\nhttps://stackoverflow.com/a/69024239 <br/>\nhttps://stackoverflow.com/a/27586132 <br />\nhttps://www.dataknowsall.com/textclustering.html <br />\nhttps://www.kaggle.com/code/naren3256/kmeans-clustering-and-cluster-visualization-in-3d/notebook <br />\nhttps://medium.com/@jwbtmf/visualizing-data-using-k-means-clustering-unsupervised-machine-learning-8b59eabfcd3d <br />\nFor choosing the appropriate number of clusters, we can use Elbow method or Silhouette score. Let's use Elbow method.","metadata":{}},{"cell_type":"code","source":"# very long\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    kmeans = KMeans(n_clusters=k)\n    # Fit model to samples\n    kmeans.fit(x2)\n    # Append the inertia to the list of inertias\n    inertias.append(kmeans.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using 7 clusters, where there's some elbow.\nn_clusters = 7\nkmeans = KMeans(n_clusters=n_clusters)\nlabels = kmeans.fit_predict(x2)\n\n# top 15 words from each cluster\norder_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = tf_vectorizer.get_feature_names_out()\nresults_dict = {}\nfor i in range(n_clusters):\n    terms_list = []\n    for ind in order_centroids[i, :15]:  \n        terms_list.append(terms[ind])\n    results_dict[f'Cluster {i}'] = terms_list\ndf_clusters = pd.DataFrame.from_dict(results_dict)\ndf_clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DBSCAN, agglomerative and spectral clustering\n\nI will leave them here for now.\n* **Davies-Bouldin Index**: a lower DBI value indicates better clustering performance, as it signifies less scatter within clusters and more separation between them.\n* **Silhouette Score**: a value that ranges from -1 to 1; a higher silhouette score indicates that the data point is well-matched to its own cluster and poorly matched to other clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import davies_bouldin_score\nfrom sklearn.metrics import silhouette_score\n\ndef print_stats_for_unsupervised(algorithm, x_data): \n    \n    y_pred = algorithm.fit_predict(x_data) \n    conf = confusion_matrix(y_pred=y_pred,y_true=np.asarray(data[\"Label\"]))\n\n    ax=plt.subplot()\n    seaborn.heatmap(conf, annot=True, fmt=\"\", linewidths=2, cmap=\"Greens\")\n    ax.set_xlabel('Predicted');\n    ax.set_ylabel('Real');\n    ax.xaxis.set_ticklabels(['Ham', 'Spam']); \n    ax.yaxis.set_ticklabels(['Ham', 'Spam']);\n    plt.show()\n    \n    print(\"Davies-Bouldin index: {:.2f}%\".format(100 * davies_bouldin_score(x_data, y_pred)))\n    print(\"Silhouette score: {:.2f}%\".format(100 * silhouette_score(x_data, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\n\nSC = SpectralClustering(n_clusters=2,assign_labels='discretize',random_state=0)\nprint_stats_for_unsupervised(SC, x2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# very long\n# from sklearn.cluster import DBSCAN\n\n# DB = DBSCAN(eps=3, min_samples=2)\n# print_stats_for_unsupervised(DB, pca_vecs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# very long\n# from sklearn.cluster import AgglomerativeClustering\n\n# AC = AgglomerativeClustering()\n# print_stats_for_unsupervised(AC, x2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Anomaly detection","metadata":{}},{"cell_type":"code","source":"# very long\n# from sklearn.ensemble import IsolationForest\n\n# IF = IsolationForest(random_state=0)\n# print_stats_for_unsupervised(IF, x2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.neighbors import LocalOutlierFactor\n\n# LOF = LocalOutlierFactor(n_neighbors=2)\n# print_stats_for_unsupervised(LOF, x2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# very long, more than 15 minutes\n# from sklearn.svm import OneClassSVM\n# OCSVM = OneClassSVM(gamma='auto')\n# print_stats_for_unsupervised(OCSVM, x2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep learning algorithms\n(subset of machine learning algorithms)<br/>\n### [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n<ul>\n    <li><b>shallow</b> vs <b>deep</b> methods, <b>feature engineering</b> (pages 17-18)</li>\n    <li><b>XGBoost</b> (gradient boosting machines) vs <b>Keras</b> (page 19)</li>\n    <li><b>History</b> object and overfitting (pages 74-76)</li>\n    <li>? <b>weight regularization</b> and <b>dropout</b> (pages 107-110)</li>\n    <li>? how do <b>2D convnets</b> and <b>MaxPooling2D</b> layer are working? (pages 122-129)</li>\n    <li>convnets and dealing with overfitting using <b>data augmentation</b> (page 159)</li>\n    <li><b>tokenization</b> (for more, look at <i>Tokenization</i> section above)</li>\n    <li><b>Embedding</b> layer (page 186)</li>\n</ul>\n\n## Supervised Learning\n**CNNs** do not use output from previous layers directly to affect future layers (apart from the standard feed-forward propagation). An example would be an image, where each pixel can be independently processed. <br />\n**RNNs** maintain a kind of 'memory' of previous inputs. This is useful in sequential data where the position and context of an element matter, like in a sentence. <br />\nhttps://pruthivi.medium.com/spam-classification-using-deep-neural-network-architecture-129860a6b9fb<br />\nhttps://www.educba.com/tensorflow-sequential/","metadata":{}},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# define the CNN model\nembedding_dim = 100  # Size of the word embeddings\n\nmodel = Sequential([\n        # first parameter input_dim=3588 == input vocab\n        Embedding(3588, embedding_dim, input_length=3588),\n        Conv1D(128, 5, activation='relu'),\n        GlobalMaxPooling1D(),\n        Dense(64, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ], name=\"cnn_model\")\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:20:17.994827Z","iopub.execute_input":"2023-07-03T14:20:17.995442Z","iopub.status.idle":"2023-07-03T14:20:18.129221Z","shell.execute_reply.started":"2023-07-03T14:20:17.995361Z","shell.execute_reply":"2023-07-03T14:20:18.127856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 10\nbatch_size = 32\nhistory = model.fit(x2_train, y2_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\nprint(f\"Test set accuracy: {accuracy:.4f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n# from tensorflow.keras.optimizers import Adam\n\n# model = Sequential()\n# model.add(Embedding(input_dim=3588, output_dim=100, input_length=3588))\n# model.add(LSTM(units=32))\n# model.add(Dense(1, activation='sigmoid'))\n\n# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:27:32.051189Z","iopub.execute_input":"2023-06-27T12:27:32.052124Z","iopub.status.idle":"2023-06-27T12:27:32.168386Z","shell.execute_reply.started":"2023-06-27T12:27:32.052059Z","shell.execute_reply":"2023-06-27T12:27:32.166382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RNN\n#### Theory\nhttps://machinelearningmastery.com/calculus-in-action-neural-networks/<br/>\nhttps://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them<br/>\nhttps://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/\n#### Tutorials\nhttps://victorzhou.com/blog/keras-rnn-tutorial/\n#### Documentation\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization<br/>\nhttps://keras.io/api/optimizers/","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import LSTM\n\nmodel = Sequential([\n        Embedding(3588, embedding_dim, input_length=3588),\n        LSTM(128),\n        Dense(10)\n    ], name=\"rnn_model\")\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T14:20:38.485102Z","iopub.execute_input":"2023-07-03T14:20:38.48556Z","iopub.status.idle":"2023-07-03T14:20:38.931168Z","shell.execute_reply.started":"2023-07-03T14:20:38.485518Z","shell.execute_reply":"2023-07-03T14:20:38.929837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nepochs = 10\nbatch_size = 32\nhistory = model.fit(x2_train, y2_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\nprint(f\"Test set accuracy: {accuracy:.4f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start_time = time.time()\n\n# # define the RNN model\n# embedding_dim = 100  # Size of the word embeddings\n\n# model = Sequential([\n#     Embedding(max_words, embedding_dim, input_length=max_length),\n#     SimpleRNN(128, activation='tanh', return_sequences=True),\n#     SimpleRNN(64, activation='tanh'),\n#     Dense(1, activation='sigmoid')\n# ])\n\n# model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n# model.summary()\n\n# # train the model\n# epochs = 10\n# batch_size = 32\n# history = model.fit(x2_train, y2_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n\n# # evaluate the model\n# loss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\n# print(f\"Test set accuracy: {accuracy:.4f}\")\n\n# end_time = time.time()\n# elapsed_time = end_time - start_time\n# print(\"Elapsed time: \", elapsed_time) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LSTM\n# start_time = time.time()\n\n# # define the LSTM model\n# embedding_dim = 100  # Size of the word embeddings\n\n# model = Sequential([\n#     Embedding(max_words, embedding_dim, input_length=max_length),\n#     LSTM(128, return_sequences=True),\n#     LSTM(64),\n#     Dense(1, activation='sigmoid')\n# ])\n\n# model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n# model.summary()\n\n# # train the model\n# epochs = 10\n# batch_size = 32\n# history = model.fit(x2_train, y2_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n\n# # evaluate the model\n# loss, accuracy = model.evaluate(x2_test, y2_test, batch_size=batch_size)\n# print(f\"Test set accuracy: {accuracy:.4f}\")\n\n# end_time = time.time()\n# elapsed_time = end_time - start_time\n# print(\"Elapsed time: \", elapsed_time) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unsupervised Learning\nOpenAI:\n> While unsupervised deep learning algorithms can help learn useful representations of the data, they typically need to be combined with a supervised classifier or clustering algorithm to perform the actual spam detection. For instance, you could use an autoencoder to learn a low-dimensional representation of the email data and then train a supervised classifier (e.g., logistic regression, SVM) on the extracted features to classify emails as spam or ham.","metadata":{}},{"cell_type":"markdown","source":"### Autoencoder","metadata":{}},{"cell_type":"code","source":"# from keras.layers import Input, Dense\n# from keras.models import Model\n\n# input_dim = x2.shape[1]\n# encoding_dim = 64  # The dimensionality of the latent space\n\n# # Define the encoder\n# input_data = Input(shape=(input_dim,))\n# encoded = Dense(encoding_dim, activation='relu')(input_data)\n\n# # Define the decoder\n# decoded = Dense(input_dim, activation='sigmoid')(encoded)\n\n# # Create the autoencoder model\n# autoencoder = Model(input_data, decoded)\n\n# # Create the encoder model\n# encoder = Model(input_data, encoded)\n\n# # Compile and train the autoencoder\n# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n# autoencoder.fit(x2, x2, epochs=50, batch_size=256, shuffle=True, \n#                 validation_data=(np.asarray(data[\"Label\"]), np.asarray(data[\"Label\"])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}